#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import os
import json
import logging
import sys
from pathlib import Path
import threading
import time
from datetime import datetime
from typing import Dict, Any, Optional, Tuple
import requests
from flask import Flask, request, jsonify
from werkzeug.utils import secure_filename
from dotenv import load_dotenv
from openai import OpenAI
from anthropic import Anthropic

# å¯é€‰ä¾èµ–çš„å ä½å¯¼å…¥ï¼ˆåœ¨è¿è¡Œç¯å¢ƒå®‰è£…åå¯ç”¨ï¼‰
try:
    from pdfminer.high_level import extract_text as pdf_extract_text
except Exception:
    pdf_extract_text = None  # type: ignore

try:
    import docx  # python-docx
except Exception:
    docx = None  # type: ignore

# åŠ è½½ç¯å¢ƒå˜é‡ï¼ˆä¼˜å…ˆåŠ è½½é¡¹ç›®æ ¹ç›®å½•çš„.env.localï¼Œç„¶å.envï¼‰
project_root = Path(__file__).parent.parent
env_local_path = project_root / ".env.local"
if env_local_path.exists():
    load_dotenv(env_local_path)
    print(f"âœ… å·²åŠ è½½é…ç½®æ–‡ä»¶: {env_local_path}")
else:
    print(f"âš ï¸  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {env_local_path}")

load_dotenv('.env.local')
load_dotenv('.env')
load_dotenv()

# é…ç½®æ—¥å¿—ï¼ˆæ”¯æŒç¯å¢ƒå˜é‡ LOG_LEVELï¼‰
_level_name = os.getenv("LOG_LEVEL", "INFO").upper()
_level = getattr(logging, _level_name, logging.INFO)
# ç»Ÿä¸€æ—¥å¿—ï¼ˆUTF-8ã€æ–‡ä»¶+æ§åˆ¶å°ã€åŒ…å«æ–‡ä»¶ä¸è¡Œå·ï¼‰
logger = logging.getLogger("resume_processor")
logger.setLevel(_level)

# æ¸…ç†é‡å¤handler
if logger.handlers:
    for h in list(logger.handlers):
        logger.removeHandler(h)

_fmt = logging.Formatter('%(asctime)s %(levelname)s %(filename)s:%(lineno)d %(message)s')

try:
    _log_file = Path(__file__).with_name('resume_processor.log')
    _fh = logging.FileHandler(_log_file, encoding='utf-8')
    _fh.setLevel(_level)
    _fh.setFormatter(_fmt)
    logger.addHandler(_fh)
except Exception:
    pass

_sh = logging.StreamHandler(sys.stdout)
_sh.setLevel(_level)
_sh.setFormatter(_fmt)
logger.addHandler(_sh)

try:
    wz = logging.getLogger('werkzeug')
    wz.setLevel(_level)
    for h in list(wz.handlers):
        wz.removeHandler(h)
    wz.addHandler(_fh) if '_fh' in globals() else None
    wz.addHandler(_sh)
except Exception:
    pass

def safe_preview(value: Any, max_len: int = 200) -> str:
    """ç”Ÿæˆå®‰å…¨å¯è¯»çš„é¢„è§ˆï¼Œæ›¿æ¢ä¸å¯æ‰“å°å­—ç¬¦ï¼Œé™åˆ¶é•¿åº¦ã€‚"""
    try:
        text = str(value)
    except Exception:
        return "<unprintable>"
    printable = []
    for ch in text:
        code = ord(ch)
        # æ”¯æŒASCIIå¯æ‰“å°å­—ç¬¦ã€Unicodeå­—ç¬¦ã€ä»¥åŠå¸¸è§çš„ç©ºç™½å­—ç¬¦
        if (32 <= code <= 126 or  # ASCIIå¯æ‰“å°å­—ç¬¦
            code >= 128 or        # Unicodeå­—ç¬¦ï¼ˆåŒ…æ‹¬ä¸­æ–‡ï¼‰
            ch in '\n\r\t '):     # å¸¸è§ç©ºç™½å­—ç¬¦
            printable.append(ch)
        else:
            printable.append('.')
    result = ''.join(printable)
    if len(result) > max_len:
        result = result[:max_len] + '...'
    return result

app = Flask(__name__)

# é…ç½®
UPLOAD_FOLDER = 'resumes'
ALLOWED_EXTENSIONS = {'txt', 'pdf', 'doc', 'docx'}
MAX_CONTENT_LENGTH = 16 * 1024 * 1024  # 16MB

# è¶…æ—¶é…ç½®ï¼ˆå¯ç”¨ç¯å¢ƒå˜é‡è¦†ç›–ï¼‰
PARSE_TIMEOUT_SEC = int(os.getenv('PARSE_TIMEOUT_SEC', '15'))
LLM_TIMEOUT_SEC = int(os.getenv('LLM_TIMEOUT_SEC', '45'))
TOTAL_TIMEOUT_SEC = int(os.getenv('TOTAL_TIMEOUT_SEC', '60'))

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = MAX_CONTENT_LENGTH

# ç¡®ä¿ä¸Šä¼ ç›®å½•å­˜åœ¨
os.makedirs(UPLOAD_FOLDER, exist_ok=True)

def allowed_file(filename):
    """æ£€æŸ¥æ–‡ä»¶ç±»å‹æ˜¯å¦å…è®¸"""
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def _extract_text_from_pdf(file_path: str) -> str:
    if not pdf_extract_text:
        logger.error("æœªå®‰è£… pdfminer.sixï¼Œæ— æ³•è§£æPDFã€‚è¯·åœ¨ ace_gtv/requirements.txt ä¸­å®‰è£… pdfminer.six")
        return ""
    try:
        text = pdf_extract_text(file_path) or ""
        logger.info(f"PDFè§£æå®Œæˆï¼Œå­—ç¬¦æ•°: {len(text)}")
        return text
    except Exception as e:
        logger.error(f"PDFè§£æå¤±è´¥: {e}")
        return ""


def _extract_text_from_docx(file_path: str) -> str:
    if not docx:
        logger.error("æœªå®‰è£… python-docxï¼Œæ— æ³•è§£æDOCXã€‚è¯·åœ¨ ace_gtv/requirements.txt ä¸­å®‰è£… python-docx")
        return ""
    try:
        d = docx.Document(file_path)
        paragraphs = [p.text for p in d.paragraphs if p.text is not None]
        text = "\n".join(paragraphs)
        logger.info(f"DOCXè§£æå®Œæˆï¼Œæ®µè½æ•°: {len(paragraphs)}ï¼Œå­—ç¬¦æ•°: {len(text)}")
        return text
    except Exception as e:
        logger.error(f"DOCXè§£æå¤±è´¥: {e}")
        return ""


def _run_with_timeout(func, args=(), kwargs=None, timeout_sec=10) -> Optional[Any]:
    """åœ¨å•ç‹¬çº¿ç¨‹æ‰§è¡Œå‡½æ•°ï¼Œè¶…æ—¶è¿”å›Noneå¹¶è®°å½•è­¦å‘Šã€‚"""
    if kwargs is None:
        kwargs = {}
    result_container = {"value": None, "error": None}

    def _target():
        try:
            result_container["value"] = func(*args, **kwargs)
        except Exception as e:
            result_container["error"] = e

    t = threading.Thread(target=_target, daemon=True)
    t.start()
    t.join(timeout=timeout_sec)
    if t.is_alive():
        logger.warning(f"ä»»åŠ¡è¶…æ—¶({timeout_sec}s): {func.__name__}")
        return None
    if result_container["error"] is not None:
        logger.error(f"ä»»åŠ¡å¼‚å¸¸: {func.__name__}: {result_container['error']}")
        return None
    return result_container["value"]


def _to_markdown(text: str) -> str:
    if not text:
        return ""
    # åŸºç¡€MarkdownåŒ–ï¼š
    lines = [ln.strip() for ln in text.splitlines()]
    md_lines = []
    for ln in lines:
        if not ln:
            md_lines.append("")
            continue
        # ç®€å•è§„åˆ™ï¼šçœ‹ä¼¼æ ‡é¢˜çš„è¡ŒåšäºŒçº§æ ‡é¢˜
        if len(ln) <= 30 and any(k in ln for k in ["å§“å", "æ•™è‚²", "æ•™è‚²èƒŒæ™¯", "ç»éªŒ", "å·¥ä½œç»éªŒ", "æŠ€èƒ½", "æˆå°±", "é¡¹ç›®", "è”ç³»æ–¹å¼", "ç”µè¯", "é‚®ç®±"]):
            md_lines.append(f"## {ln}")
        else:
            md_lines.append(ln)
    return "\n".join(md_lines)


def _save_markdown_alongside(src_path: str, markdown_text: str) -> Optional[str]:
    try:
        md_path = str(Path(src_path).with_suffix('.md'))
        with open(md_path, 'w', encoding='utf-8') as f:
            f.write(markdown_text)
        logger.info(f"å·²ä¿å­˜Markdownå†…å®¹: {md_path}")
        return md_path
    except Exception as e:
        logger.error(f"ä¿å­˜Markdownå¤±è´¥: {e}")
        return None


def extract_text_from_file(file_path: str) -> str:
    """ä»æ–‡ä»¶ä¸­æå–æ–‡æœ¬å†…å®¹"""
    logger.info(f"å¼€å§‹æå–æ–‡ä»¶æ–‡æœ¬å†…å®¹: {file_path}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(file_path):
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return ""
    
    # è·å–æ–‡ä»¶å¤§å°
    file_size = os.path.getsize(file_path)
    logger.info(f"æ–‡ä»¶å¤§å°: {file_size} bytes")
    
    # æ ¹æ®æ‰©å±•åä¼˜å…ˆä½¿ç”¨ä¸“ç”¨è§£æå™¨
    suffix = Path(file_path).suffix.lower()
    if suffix == '.pdf':
        logger.info("æ£€æµ‹åˆ°PDFæ–‡ä»¶ï¼Œä½¿ç”¨pdfminerè§£æ")
        text_pdf = _run_with_timeout(_extract_text_from_pdf, args=(file_path,), timeout_sec=PARSE_TIMEOUT_SEC)
        if text_pdf is None:
            logger.error("PDFè§£æè¶…æ—¶æˆ–å¤±è´¥ï¼Œå»ºè®®è½¬æ¢ä¸ºæ–‡æœ¬å‹PDF/ä¸Šä¼ TXTã€‚")
            return ""
        md_pdf = _to_markdown(text_pdf)
        _save_markdown_alongside(file_path, md_pdf)
        return text_pdf
    if suffix == '.docx':
        logger.info("æ£€æµ‹åˆ°DOCXæ–‡ä»¶ï¼Œä½¿ç”¨python-docxè§£æ")
        text_docx = _run_with_timeout(_extract_text_from_docx, args=(file_path,), timeout_sec=PARSE_TIMEOUT_SEC)
        if text_docx is None:
            logger.error("DOCXè§£æè¶…æ—¶æˆ–å¤±è´¥ï¼Œå»ºè®®è½¬æ¢ä¸ºDOCX(æ–‡æœ¬)æˆ–PDF/TXTã€‚")
            return ""
        md_docx = _to_markdown(text_docx)
        _save_markdown_alongside(file_path, md_docx)
        return text_docx
    if suffix == '.doc':
        logger.warning("æ£€æµ‹åˆ°DOC(97-2003)æ–‡ä»¶ï¼Œå½“å‰æœªå†…ç½®è§£æå™¨ã€‚å»ºè®®è½¬æ¢ä¸ºDOCXæˆ–PDFåå†ä¸Šä¼ ã€‚å°è¯•æŒ‰æ–‡æœ¬è¯»å–ã€‚")

    # å°è¯•å¤šç§ç¼–ç æ ¼å¼ï¼ˆç”¨äºtxt/æœªçŸ¥åœºæ™¯ï¼‰
    encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1', 'cp1252', 'iso-8859-1']
    logger.info(f"å°è¯•ä½¿ç”¨ {len(encodings)} ç§ç¼–ç æ ¼å¼è¯»å–æ–‡ä»¶")
    
    for i, encoding in enumerate(encodings, 1):
        try:
            logger.debug(f"å°è¯•ç¼–ç  {i}/{len(encodings)}: {encoding}")
            with open(file_path, 'r', encoding=encoding) as f:
                content = f.read()
                logger.info(f"æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç è¯»å–æ–‡ä»¶ï¼Œå†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                # ç®€å•å¥åº·æ£€æŸ¥ï¼šæ£€æµ‹ç–‘ä¼¼äºŒè¿›åˆ¶/OfficeåŒ…ç­¾å
                if '\u0000' in content[:200] or 'PK\x03\x04' in content[:200]:
                    logger.warning("æ£€æµ‹åˆ°ç–‘ä¼¼äºŒè¿›åˆ¶/Officeå‹ç¼©æ ¼å¼ç­¾åï¼Œå†…å®¹å¯èƒ½ä¸æ˜¯çº¯æ–‡æœ¬ã€‚å»ºè®®è½¬æ¢ä¸ºTXT/PDFåå†ä¸Šä¼ ã€‚")
                # æ–‡æœ¬->Markdownå¹¶ä¿å­˜
                _save_markdown_alongside(file_path, _to_markdown(content))
                return content
        except UnicodeDecodeError as e:
            logger.debug(f"ç¼–ç  {encoding} å¤±è´¥ (UnicodeDecodeError): {e}")
            continue
        except Exception as e:
            logger.error(f"ä½¿ç”¨ {encoding} ç¼–ç è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            continue
    
    # å¦‚æœæ‰€æœ‰ç¼–ç éƒ½å¤±è´¥ï¼Œå°è¯•ä»¥äºŒè¿›åˆ¶æ–¹å¼è¯»å–å¹¶å¿½ç•¥é”™è¯¯
    logger.warning("æ‰€æœ‰ç¼–ç æ–¹å¼éƒ½å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨UTF-8ç¼–ç å¹¶å¿½ç•¥é”™è¯¯å­—ç¬¦")
    try:
        with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
            logger.warning(f"ä½¿ç”¨UTF-8ç¼–ç å¹¶å¿½ç•¥é”™è¯¯å­—ç¬¦è¯»å–æ–‡ä»¶æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
            _save_markdown_alongside(file_path, _to_markdown(content))
            return content
    except Exception as e:
        logger.error(f"æ‰€æœ‰ç¼–ç æ–¹å¼éƒ½å¤±è´¥: {e}")
        return ""

def _get_llm_client() -> Optional[Any]:
    """è·å–é…ç½®çš„LLMå®¢æˆ·ç«¯å®ä¾‹ã€‚"""
    # å…¼å®¹æ€§å¤„ç†ï¼šæ”¯æŒAI_PROVIDERå’ŒLLM_PROVIDERä¸¤ç§é…ç½®æ–¹å¼
    ai_provider = os.getenv("AI_PROVIDER", "").lower()
    llm_provider = os.getenv("LLM_PROVIDER", "").upper()
    
    if ai_provider:
        # ä¼˜å…ˆä½¿ç”¨AI_PROVIDERçš„å€¼ï¼Œè¦†ç›–LLM_PROVIDER
        if ai_provider == "openai":
            os.environ["LLM_PROVIDER"] = "OPENAI"
        elif ai_provider == "azure":
            os.environ["LLM_PROVIDER"] = "AZURE"
        elif ai_provider == "anthropic":
            os.environ["LLM_PROVIDER"] = "ANTHROPIC"
        print(f"ğŸ”„ ä½¿ç”¨AI_PROVIDER={ai_provider} -> LLM_PROVIDER={os.environ['LLM_PROVIDER']}")
    
    # å…¼å®¹æ€§å¤„ç†ï¼šæ”¯æŒAZURE_API_KEYå’ŒAZURE_OPENAI_API_KEY
    azure_api_key = os.getenv("AZURE_API_KEY")
    azure_openai_api_key = os.getenv("AZURE_OPENAI_API_KEY")
    if azure_api_key and not azure_openai_api_key:
        os.environ["AZURE_OPENAI_API_KEY"] = azure_api_key
        logger.info("è‡ªåŠ¨æ˜ å°„AZURE_API_KEY -> AZURE_OPENAI_API_KEY")
    
    # å…¼å®¹æ€§å¤„ç†ï¼šæ”¯æŒAZURE_OPENAI_DEPLOYMENT_NAMEå’ŒAZURE_OPENAI_DEPLOYMENT
    azure_deployment_name = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")
    azure_deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT")
    if azure_deployment_name and not azure_deployment:
        os.environ["AZURE_OPENAI_DEPLOYMENT"] = azure_deployment_name
        logger.info(f"è‡ªåŠ¨æ˜ å°„AZURE_OPENAI_DEPLOYMENT_NAME -> AZURE_OPENAI_DEPLOYMENT")
    
    provider = os.getenv("LLM_PROVIDER", "OPENAI").upper()
    
    if provider == "OPENAI":
        api_key = os.getenv("OPENAI_API_KEY")
        base_url = os.getenv("OPENAI_API_BASE", "https://api.openai.com/v1")
        if not api_key:
            logger.warning("æœªé…ç½®OPENAI_API_KEYï¼Œè·³è¿‡LLMæå–")
            return None
        return OpenAI(api_key=api_key, base_url=base_url)
    
    elif provider == "AZURE":
        endpoint = os.getenv("AZURE_OPENAI_ENDPOINT", "").rstrip("/")
        api_key = os.getenv("AZURE_OPENAI_API_KEY", "")
        api_version = os.getenv("AZURE_OPENAI_API_VERSION", "2024-02-15-preview")
        if not (endpoint and api_key):
            logger.warning("æœªé…ç½®Azure OpenAIå‚æ•°ï¼Œè·³è¿‡LLMæå–")
            return None
        return OpenAI(
            api_key=api_key,
            base_url=f"{endpoint}/openai/deployments/{os.getenv('AZURE_OPENAI_DEPLOYMENT', '')}",
            default_query={"api-version": api_version}
        )
    
    elif provider == "ANTHROPIC":
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            logger.warning("æœªé…ç½®ANTHROPIC_API_KEYï¼Œè·³è¿‡LLMæå–")
            return None
        return Anthropic(api_key=api_key)
    
    else:
        logger.warning(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {provider}")
        return None


def _parse_llm_json(text: str) -> Dict[str, Any]:
    cleaned = text.strip()
    if cleaned.startswith("```json"):
        cleaned = cleaned[7:]
    if cleaned.startswith("```"):
        cleaned = cleaned[3:]
    if cleaned.endswith("```"):
        cleaned = cleaned[:-3]
    try:
        return json.loads(cleaned)
    except Exception as e:
        logger.error(f"LLM JSONè§£æå¤±è´¥: {e}; é¢„è§ˆ: {safe_preview(cleaned)}")
        raise


def call_ai_for_extraction(content: str) -> Dict[str, Any]:
    """ä¼˜å…ˆè°ƒç”¨LLMè¿›è¡Œä¿¡æ¯æå–ï¼›å¤±è´¥åˆ™å›é€€æœ¬åœ°è§„åˆ™ã€‚"""
    logger.info(f"å¼€å§‹AIä¿¡æ¯æå–ï¼Œè¾“å…¥å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
    
    client = _get_llm_client()
    if not client:
        logger.info("æœªé…ç½®LLMå®¢æˆ·ç«¯ï¼Œå›é€€åˆ°æœ¬åœ°è§„åˆ™æå–")
        return _extract_with_local_rules(content)
    
    try:
        provider = os.getenv("LLM_PROVIDER", "OPENAI").upper()
        logger.info(f"è°ƒç”¨LLMæä¾›å•†: {provider}")
        
        system_prompt = (
            "ä½ æ˜¯èµ„æ·±ç­¾è¯é¡¾é—®ï¼Œè¯·ä»ç®€å†å…¨æ–‡ä¸­æç‚¼ç»“æ„åŒ–ä¿¡æ¯ã€‚"
            "ä¸¥æ ¼è¿”å›JSONå¯¹è±¡ï¼Œä¸è¦åŒ…å«å¤šä½™è¯´æ˜æˆ–Markdownå›´æ ã€‚"
        )
        user_prompt = (
            "è¯·ä»ä»¥ä¸‹ç®€å†å†…å®¹ä¸­æå–: name, email, phone, experience(è¿ç»­æ–‡æœ¬),"
            "education(è¿ç»­æ–‡æœ¬), skills(æ•°ç»„), achievements(æ•°ç»„), projects(æ•°ç»„),"
            "languages(æ•°ç»„), certifications(æ•°ç»„), summary(æ‘˜è¦)ã€‚\n\nç®€å†å…¨æ–‡:\n" + content
        )

        if provider in ["OPENAI", "AZURE"]:
            # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯
            model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
            if provider == "AZURE":
                deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT", "")
                if not deployment:
                    raise ValueError("Azure OpenAIéœ€è¦é…ç½®AZURE_OPENAI_DEPLOYMENT")
                model = deployment
            
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=0.2,
                timeout=LLM_TIMEOUT_SEC
            )
            llm_text = response.choices[0].message.content
            
        elif provider == "ANTHROPIC":
            # ä½¿ç”¨Anthropicå®¢æˆ·ç«¯
            response = client.messages.create(
                model="claude-3-sonnet-20240229",
                max_tokens=1500,
                temperature=0.2,
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}],
                timeout=LLM_TIMEOUT_SEC
            )
            llm_text = response.content[0].text
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {provider}")

        logger.info(f"LLMè¿”å›æ–‡æœ¬é•¿åº¦: {len(llm_text)}; é¢„è§ˆ: {safe_preview(llm_text)}")
        parsed = _parse_llm_json(llm_text)

        # å…œåº•å¡«å……ä¸ç±»å‹è§„æ•´
        extracted = {
            "name": parsed.get("name") or "",
            "email": parsed.get("email") or "",
            "phone": parsed.get("phone") or "",
            "experience": parsed.get("experience") or "",
            "education": parsed.get("education") or "",
            "skills": parsed.get("skills") or [],
            "achievements": parsed.get("achievements") or [],
            "projects": parsed.get("projects") or [],
            "languages": parsed.get("languages") or [],
            "certifications": parsed.get("certifications") or [],
            "summary": parsed.get("summary") or "",
        }
        logger.info("LLMä¿¡æ¯æå–æˆåŠŸ")
        return extracted
    except Exception as e:
        logger.error(f"LLMè°ƒç”¨å¤±è´¥ï¼Œå›é€€æœ¬åœ°è§„åˆ™: {e}", exc_info=True)
        return _extract_with_local_rules(content)


def call_ai_for_gtv_assessment(extracted_info: Dict[str, Any], field: str) -> Dict[str, Any]:
    """ä½¿ç”¨LLMè¿›è¡ŒGTVèµ„æ ¼è¯„ä¼°"""
    logger.info(f"å¼€å§‹GTVèµ„æ ¼è¯„ä¼°ï¼Œé¢†åŸŸ: {field}")
    
    client = _get_llm_client()
    if not client:
        logger.warning("æœªé…ç½®LLMå®¢æˆ·ç«¯ï¼Œä½¿ç”¨é»˜è®¤GTVè¯„ä¼°")
        return _get_default_gtv_assessment(extracted_info, field)
    
    try:
        provider = os.getenv("LLM_PROVIDER", "OPENAI").upper()
        logger.info(f"è°ƒç”¨LLMæä¾›å•†è¿›è¡ŒGTVè¯„ä¼°: {provider}")
        
        # æ„å»ºè¯„ä¼°æç¤º
        system_prompt = (
            "ä½ æ˜¯è‹±å›½Global Talent Visa (GTV) çš„èµ„æ·±è¯„ä¼°ä¸“å®¶ã€‚"
            "è¯·åŸºäºç”³è¯·äººçš„ç®€å†ä¿¡æ¯ï¼Œè¿›è¡Œå…¨é¢çš„GTVèµ„æ ¼è¯„ä¼°ã€‚"
            "è¯„ä¼°æ ‡å‡†åŒ…æ‹¬ï¼šæ°å‡ºäººæ‰ã€åˆ›æ–°è´¡çŒ®ã€è¡Œä¸šå½±å“åŠ›ã€é¢†å¯¼åŠ›ç­‰ã€‚"
            "ä¸¥æ ¼è¿”å›JSONå¯¹è±¡ï¼Œä¸è¦åŒ…å«å¤šä½™è¯´æ˜æˆ–Markdownå›´æ ã€‚"
        )
        
        # æ ¹æ®é¢†åŸŸè°ƒæ•´è¯„ä¼°é‡ç‚¹
        field_focus = {
            "digital-technology": "æ•°å­—æŠ€æœ¯ã€äººå·¥æ™ºèƒ½ã€è½¯ä»¶å·¥ç¨‹ã€æŠ€æœ¯åˆ›æ–°",
            "arts-culture": "è‰ºæœ¯åˆ›ä½œã€æ–‡åŒ–è´¡çŒ®ã€åˆ›æ„äº§ä¸šã€æ–‡åŒ–å½±å“åŠ›",
            "research-academia": "å­¦æœ¯ç ”ç©¶ã€ç§‘å­¦å‘ç°ã€æ•™è‚²è´¡çŒ®ã€å­¦æœ¯å£°èª‰"
        }
        
        field_description = field_focus.get(field, "ç»¼åˆé¢†åŸŸ")
        
        user_prompt = f"""
è¯·åŸºäºä»¥ä¸‹ç”³è¯·äººä¿¡æ¯è¿›è¡ŒGTVèµ„æ ¼è¯„ä¼°ï¼š

ç”³è¯·äººä¿¡æ¯ï¼š
- å§“å: {extracted_info.get('name', 'N/A')}
- æ•™è‚²èƒŒæ™¯: {extracted_info.get('education', 'N/A')}
- å·¥ä½œç»éªŒ: {extracted_info.get('experience', 'N/A')}
- æŠ€èƒ½ä¸“é•¿: {', '.join(extracted_info.get('skills', []))}
- ä¸»è¦æˆå°±: {', '.join(extracted_info.get('achievements', []))}
- é¡¹ç›®ç»éªŒ: {', '.join(extracted_info.get('projects', []))}
- è®¤è¯èµ„è´¨: {', '.join(extracted_info.get('certifications', []))}
- ä¸ªäººæ€»ç»“: {extracted_info.get('summary', 'N/A')}

è¯„ä¼°é¢†åŸŸ: {field_description}

è¯·è¿”å›ä»¥ä¸‹ç»“æ„çš„JSONè¯„ä¼°ç»“æœï¼š
{{
  "applicantInfo": {{
    "name": "ç”³è¯·äººå§“å",
    "field": "ç”³è¯·é¢†åŸŸ",
    "currentPosition": "å½“å‰èŒä½",
    "company": "å½“å‰å…¬å¸",
    "yearsOfExperience": "å·¥ä½œç»éªŒå¹´æ•°"
  }},
  "educationBackground": {{
    "degrees": ["å­¦ä½åˆ—è¡¨"],
    "institutions": ["å­¦æ ¡åˆ—è¡¨"],
    "analysis": "æ•™è‚²èƒŒæ™¯åˆ†æ"
  }},
  "industryBackground": {{
    "sector": "è¡Œä¸šé¢†åŸŸ",
    "yearsInIndustry": "è¡Œä¸šç»éªŒå¹´æ•°",
    "keyCompanies": ["å…³é”®å…¬å¸åˆ—è¡¨"],
    "industryImpact": è¡Œä¸šå½±å“åŠ›è¯„åˆ†(1-10),
    "analysis": "è¡Œä¸šèƒŒæ™¯åˆ†æ"
  }},
  "workExperience": {{
    "positions": ["èŒä½åˆ—è¡¨"],
    "keyAchievements": ["å…³é”®æˆå°±åˆ—è¡¨"],
    "leadershipRoles": ["é¢†å¯¼è§’è‰²åˆ—è¡¨"],
    "projectImpact": ["é¡¹ç›®å½±å“åˆ—è¡¨"],
    "analysis": "å·¥ä½œç»éªŒåˆ†æ"
  }},
  "technicalExpertise": {{
    "coreSkills": ["æ ¸å¿ƒæŠ€èƒ½åˆ—è¡¨"],
    "specializations": ["ä¸“ä¸šé¢†åŸŸåˆ—è¡¨"],
    "innovations": ["åˆ›æ–°æˆæœåˆ—è¡¨"],
    "industryRecognition": ["è¡Œä¸šè®¤å¯åˆ—è¡¨"],
    "analysis": "æŠ€æœ¯ä¸“é•¿åˆ†æ"
  }},
  "gtvPathway": {{
    "recommendedRoute": "æ¨èè·¯å¾„(Exceptional Talent/Exceptional Promise)",
    "eligibilityLevel": "èµ„æ ¼ç­‰çº§(Strong/Good/Weak)",
    "yearsOfExperience": "ç›¸å…³ç»éªŒå¹´æ•°",
    "analysis": "GTVè·¯å¾„åˆ†æ"
  }},
  "strengths": [
    {{
      "area": "ä¼˜åŠ¿é¢†åŸŸ",
      "description": "ä¼˜åŠ¿æè¿°",
      "evidence": "è¯æ®æ”¯æ’‘"
    }}
  ],
  "weaknesses": [
    {{
      "area": "éœ€è¦æ”¹è¿›çš„é¢†åŸŸ",
      "description": "æ”¹è¿›æè¿°",
      "improvement": "æ”¹è¿›å»ºè®®",
      "priority": "ä¼˜å…ˆçº§(High/Medium/Low)"
    }}
  ],
  "criteriaAssessment": [
    {{
      "name": "è¯„ä¼°æ ‡å‡†åç§°",
      "status": "çŠ¶æ€(Met/Partially Met/Not Met)",
      "score": è¯„åˆ†(0-100),
      "evidence": "è¯„ä¼°è¯æ®"
    }}
  ],
  "overallScore": æ€»ä½“è¯„åˆ†(0-100),
  "recommendation": "ç”³è¯·å»ºè®®",
  "professionalAdvice": ["ä¸“ä¸šå»ºè®®åˆ—è¡¨"],
  "timeline": "ç”³è¯·æ—¶é—´çº¿",
  "requiredDocuments": ["æ‰€éœ€æ–‡æ¡£åˆ—è¡¨"],
  "estimatedBudget": {{
    "min": æœ€ä½é¢„ç®—,
    "max": æœ€é«˜é¢„ç®—,
    "currency": "è´§å¸å•ä½"
  }}
}}
"""

        if provider in ["OPENAI", "AZURE"]:
            # ä½¿ç”¨OpenAIå®¢æˆ·ç«¯
            model = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
            if provider == "AZURE":
                deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT", "")
                if not deployment:
                    raise ValueError("Azure OpenAIéœ€è¦é…ç½®AZURE_OPENAI_DEPLOYMENT")
                model = deployment
            
            response = client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=0.3,
                timeout=LLM_TIMEOUT_SEC
            )
            llm_text = response.choices[0].message.content
            
        elif provider == "ANTHROPIC":
            # ä½¿ç”¨Anthropicå®¢æˆ·ç«¯
            response = client.messages.create(
                model="claude-3-sonnet-20240229",
                max_tokens=2000,
                temperature=0.3,
                system=system_prompt,
                messages=[{"role": "user", "content": user_prompt}],
                timeout=LLM_TIMEOUT_SEC
            )
            llm_text = response.content[0].text
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„LLMæä¾›å•†: {provider}")

        logger.info(f"GTVè¯„ä¼°LLMè¿”å›æ–‡æœ¬é•¿åº¦: {len(llm_text)}; é¢„è§ˆ: {safe_preview(llm_text)}")
        parsed = _parse_llm_json(llm_text)
        
        logger.info("GTVèµ„æ ¼è¯„ä¼°æˆåŠŸ")
        return parsed
        
    except Exception as e:
        logger.error(f"GTVè¯„ä¼°LLMè°ƒç”¨å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è¯„ä¼°: {e}", exc_info=True)
        return _get_default_gtv_assessment(extracted_info, field)


def _get_default_gtv_assessment(extracted_info: Dict[str, Any], field: str) -> Dict[str, Any]:
    """é»˜è®¤GTVè¯„ä¼°ï¼ˆå½“LLMä¸å¯ç”¨æ—¶ï¼‰"""
    logger.info("ä½¿ç”¨é»˜è®¤GTVè¯„ä¼°")
    
    field_mapping = {
        "digital-technology": "Digital Technology",
        "arts-culture": "Arts & Culture", 
        "research-academia": "Research & Academia"
    }
    
    return {
        "applicantInfo": {
            "name": extracted_info.get("name", "N/A"),
            "field": field_mapping.get(field, "Digital Technology"),
            "currentPosition": "å¾…ç¡®å®š",
            "company": "å¾…ç¡®å®š",
            "yearsOfExperience": "å¾…ç¡®å®š"
        },
        "educationBackground": {
            "degrees": [extracted_info.get("education", "å¾…ç¡®å®š")],
            "institutions": ["å¾…ç¡®å®š"],
            "analysis": "æ•™è‚²èƒŒæ™¯éœ€è¦è¿›ä¸€æ­¥åˆ†æ"
        },
        "industryBackground": {
            "sector": "å¾…ç¡®å®š",
            "yearsInIndustry": "å¾…ç¡®å®š",
            "keyCompanies": ["å¾…ç¡®å®š"],
            "industryImpact": 5,
            "analysis": "è¡Œä¸šèƒŒæ™¯éœ€è¦è¿›ä¸€æ­¥åˆ†æ"
        },
        "workExperience": {
            "positions": [extracted_info.get("experience", "å¾…ç¡®å®š")],
            "keyAchievements": extracted_info.get("achievements", []),
            "leadershipRoles": ["å¾…ç¡®å®š"],
            "projectImpact": extracted_info.get("projects", []),
            "analysis": "å·¥ä½œç»éªŒéœ€è¦è¿›ä¸€æ­¥åˆ†æ"
        },
        "technicalExpertise": {
            "coreSkills": extracted_info.get("skills", []),
            "specializations": ["å¾…ç¡®å®š"],
            "innovations": extracted_info.get("projects", []),
            "industryRecognition": extracted_info.get("achievements", []),
            "analysis": "æŠ€æœ¯ä¸“é•¿éœ€è¦è¿›ä¸€æ­¥åˆ†æ"
        },
        "gtvPathway": {
            "recommendedRoute": "å¾…è¯„ä¼°",
            "eligibilityLevel": "å¾…è¯„ä¼°",
            "yearsOfExperience": "å¾…ç¡®å®š",
            "analysis": "éœ€è¦LLMè¿›è¡Œè¯¦ç»†è¯„ä¼°"
        },
        "strengths": [
            {
                "area": "æŠ€èƒ½ä¸“é•¿",
                "description": "å…·å¤‡ç›¸å…³æŠ€èƒ½",
                "evidence": "ç®€å†ä¸­çš„æŠ€èƒ½åˆ—è¡¨"
            }
        ],
        "weaknesses": [
            {
                "area": "è¯„ä¼°å®Œæ•´æ€§",
                "description": "éœ€è¦æ›´è¯¦ç»†çš„è¯„ä¼°",
                "improvement": "å»ºè®®é…ç½®LLMè¿›è¡Œæ™ºèƒ½è¯„ä¼°",
                "priority": "High"
            }
        ],
        "criteriaAssessment": [
            {
                "name": "åŸºç¡€èµ„æ ¼",
                "status": "Partially Met",
                "score": 50,
                "evidence": "åŸºç¡€ä¿¡æ¯å·²æå–ï¼Œéœ€è¦è¿›ä¸€æ­¥è¯„ä¼°"
            }
        ],
        "overallScore": 50,
        "recommendation": "å»ºè®®é…ç½®LLMè¿›è¡Œè¯¦ç»†è¯„ä¼°ä»¥è·å¾—æ›´å‡†ç¡®çš„ç»“æœ",
        "professionalAdvice": [
            "é…ç½®LLMæœåŠ¡ä»¥è·å¾—æ™ºèƒ½è¯„ä¼°",
            "æä¾›æ›´è¯¦ç»†çš„ç®€å†ä¿¡æ¯",
            "å‡†å¤‡ç›¸å…³æ”¯æŒæ–‡æ¡£"
        ],
        "timeline": "å¾…è¯„ä¼°",
        "requiredDocuments": [
            "ç®€å†/CV",
            "å­¦å†è¯æ˜",
            "å·¥ä½œè¯æ˜",
            "æ¨èä¿¡"
        ],
        "estimatedBudget": {
            "min": 50000,
            "max": 100000,
            "currency": "GBP"
        }
    }


def _extract_with_local_rules(content: str) -> Dict[str, Any]:
    """æœ¬åœ°è§„åˆ™ä¿¡æ¯æå–ï¼ˆå›é€€æœºåˆ¶ï¼‰"""
    logger.info("æ‰§è¡Œæœ¬åœ°è§„åˆ™ä¿¡æ¯æå–")
    try:
        lines = content.split('\n')
        logger.info(f"å›é€€è§„åˆ™ï¼šå°†å†…å®¹åˆ†å‰²ä¸º {len(lines)} è¡Œè¿›è¡Œå¤„ç†")

        extracted_info = {
            "name": "",
            "email": "",
            "phone": "",
            "experience": "",
            "education": "",
            "skills": [],
            "achievements": [],
            "projects": [],
            "languages": [],
            "certifications": [],
            "summary": ""
        }

        processed_lines = 0
        for line_num, line in enumerate(lines, 1):
            line = line.strip()
            if not line:
                continue
            processed_lines += 1
            if not extracted_info["name"] and len(line) < 20 and not any(keyword in line.lower() for keyword in ['@', 'ç”µè¯', 'é‚®ç®±', 'æŠ€èƒ½', 'ç»éªŒ', 'æ•™è‚²']):
                extracted_info["name"] = line
            if '@' in line and 'email' not in line.lower():
                extracted_info["email"] = line
            if any(char.isdigit() for char in line) and ('ç”µè¯' in line or '+' in line or '-' in line):
                extracted_info["phone"] = line
            if 'æŠ€èƒ½' in line or 'skills' in line.lower():
                skills_text = line.replace('æŠ€èƒ½', '').replace('skills', '').replace(':', '').strip()
                if skills_text:
                    extracted_info["skills"] = [skill.strip() for skill in skills_text.split(',') if skill.strip()]
            if 'æˆå°±' in line or 'achievements' in line.lower():
                achievements_text = line.replace('æˆå°±', '').replace('achievements', '').replace(':', '').strip()
                if achievements_text:
                    extracted_info["achievements"] = [ach.strip() for ach in achievements_text.split(',') if ach.strip()]

        # ç®€å•æ®µè½æå–
        experience_lines = []
        in_experience = False
        for line in lines:
            if 'å·¥ä½œç»éªŒ' in line or 'experience' in line.lower():
                in_experience = True
                continue
            elif in_experience and ('æ•™è‚²' in line or 'education' in line.lower() or 'æŠ€èƒ½' in line or 'skills' in line.lower()):
                break
            elif in_experience and line:
                experience_lines.append(line)
        if experience_lines:
            extracted_info["experience"] = ' '.join(experience_lines)

        education_lines = []
        in_education = False
        for line in lines:
            if 'æ•™è‚²' in line or 'education' in line.lower():
                in_education = True
                continue
            elif in_education and ('æŠ€èƒ½' in line or 'skills' in line.lower() or 'æˆå°±' in line or 'achievements' in line.lower()):
                break
            elif in_education and line:
                education_lines.append(line)
        if education_lines:
            extracted_info["education"] = ' '.join(education_lines)

        logger.info("æœ¬åœ°è§„åˆ™ä¿¡æ¯æå–å®Œæˆ")
        return extracted_info
    except Exception as e:
        logger.error(f"æœ¬åœ°è§„åˆ™æå–å¤±è´¥: {e}", exc_info=True)
        return {}

def create_personal_knowledge_base(name: str, extracted_info: Dict[str, Any]) -> str:
    """ä¸ºä¸ªäººåˆ›å»ºçŸ¥è¯†åº“"""
    logger.info(f"å¼€å§‹ä¸º {name} åˆ›å»ºä¸ªäººçŸ¥è¯†åº“")
    logger.info(f"æå–çš„ä¿¡æ¯: {extracted_info}")
    
    try:
        # åˆ›å»ºä¸ªäººçŸ¥è¯†åº“ç›®å½•
        personal_dir = Path(f"personal_kb/{secure_filename(name)}")
        logger.info(f"åˆ›å»ºä¸ªäººçŸ¥è¯†åº“ç›®å½•: {personal_dir}")
        personal_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"ä¸ªäººçŸ¥è¯†åº“ç›®å½•åˆ›å»ºæˆåŠŸ")
        
        # ä¿å­˜ä¸ªäººä¿¡æ¯
        personal_info = {
            "name": name,
            "created_at": datetime.now().isoformat(),
            "last_updated": datetime.now().isoformat(),
            "extracted_info": extracted_info,
            "knowledge_bullets": []
        }
        logger.info(f"åˆå§‹åŒ–ä¸ªäººä¿¡æ¯ç»“æ„")
        
        # æ ¹æ®æå–çš„ä¿¡æ¯åˆ›å»ºçŸ¥è¯†æ¡ç›®
        knowledge_bullets = []
        logger.info("å¼€å§‹æ ¹æ®æå–çš„ä¿¡æ¯åˆ›å»ºçŸ¥è¯†æ¡ç›®")
        
        if extracted_info.get("experience"):
            bullet = {
                "id": f"{name}_exp_1",
                "section": "å·¥ä½œç»éªŒ",
                "content": extracted_info["experience"],
                "helpful": True,
                "harmful": False,
                "metadata": {"source": "resume", "type": "experience"}
            }
            knowledge_bullets.append(bullet)
            logger.info(f"åˆ›å»ºå·¥ä½œç»éªŒçŸ¥è¯†æ¡ç›®: {bullet['id']}")
            
        if extracted_info.get("education"):
            bullet = {
                "id": f"{name}_edu_1",
                "section": "æ•™è‚²èƒŒæ™¯",
                "content": extracted_info["education"],
                "helpful": True,
                "harmful": False,
                "metadata": {"source": "resume", "type": "education"}
            }
            knowledge_bullets.append(bullet)
            logger.info(f"åˆ›å»ºæ•™è‚²èƒŒæ™¯çŸ¥è¯†æ¡ç›®: {bullet['id']}")
            
        if extracted_info.get("skills"):
            logger.info(f"åˆ›å»ºæŠ€èƒ½çŸ¥è¯†æ¡ç›®ï¼ŒæŠ€èƒ½æ•°é‡: {len(extracted_info['skills'])}")
            for i, skill in enumerate(extracted_info["skills"]):
                bullet = {
                    "id": f"{name}_skill_{i+1}",
                    "section": "æŠ€èƒ½ä¸“é•¿",
                    "content": skill,
                    "helpful": True,
                    "harmful": False,
                    "metadata": {"source": "resume", "type": "skill"}
                }
                knowledge_bullets.append(bullet)
                logger.info(f"åˆ›å»ºæŠ€èƒ½çŸ¥è¯†æ¡ç›® {i+1}: {bullet['id']} - {skill}")
                
        if extracted_info.get("achievements"):
            logger.info(f"åˆ›å»ºæˆå°±çŸ¥è¯†æ¡ç›®ï¼Œæˆå°±æ•°é‡: {len(extracted_info['achievements'])}")
            for i, achievement in enumerate(extracted_info["achievements"]):
                bullet = {
                    "id": f"{name}_ach_{i+1}",
                    "section": "æˆå°±è£èª‰",
                    "content": achievement,
                    "helpful": True,
                    "harmful": False,
                    "metadata": {"source": "resume", "type": "achievement"}
                }
                knowledge_bullets.append(bullet)
                logger.info(f"åˆ›å»ºæˆå°±çŸ¥è¯†æ¡ç›® {i+1}: {bullet['id']} - {achievement}")
        
        personal_info["knowledge_bullets"] = knowledge_bullets
        logger.info(f"çŸ¥è¯†æ¡ç›®åˆ›å»ºå®Œæˆï¼Œæ€»è®¡ {len(knowledge_bullets)} ä¸ªæ¡ç›®")
        
        # ä¿å­˜åˆ°æ–‡ä»¶
        personal_file = personal_dir / "personal_info.json"
        logger.info(f"ä¿å­˜ä¸ªäººçŸ¥è¯†åº“åˆ°æ–‡ä»¶: {personal_file}")
        with open(personal_file, 'w', encoding='utf-8') as f:
            json.dump(personal_info, f, ensure_ascii=False, indent=2)
        logger.info(f"ä¸ªäººçŸ¥è¯†åº“æ–‡ä»¶ä¿å­˜æˆåŠŸ")
            
        logger.info(f"ä¸º {name} åˆ›å»ºäº†ä¸ªäººçŸ¥è¯†åº“ï¼ŒåŒ…å« {len(knowledge_bullets)} ä¸ªçŸ¥è¯†æ¡ç›®")
        return str(personal_dir)
        
    except Exception as e:
        logger.error(f"åˆ›å»ºä¸ªäººçŸ¥è¯†åº“å¤±è´¥: {e}", exc_info=True)
        return ""

def update_main_knowledge_base(personal_kb_path: str, name: str) -> bool:
    """å°†ä¸ªäººçŸ¥è¯†åº“æ›´æ–°åˆ°ä¸»çŸ¥è¯†åº“"""
    logger.info(f"å¼€å§‹æ›´æ–°ä¸»çŸ¥è¯†åº“ï¼Œä¸ªäººçŸ¥è¯†åº“è·¯å¾„: {personal_kb_path}, å§“å: {name}")
    
    try:
        # è¯»å–ä¸ªäººçŸ¥è¯†åº“
        personal_file = Path(personal_kb_path) / "personal_info.json"
        logger.info(f"è¯»å–ä¸ªäººçŸ¥è¯†åº“æ–‡ä»¶: {personal_file}")
        
        if not personal_file.exists():
            logger.error(f"ä¸ªäººçŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨: {personal_file}")
            return False
            
        with open(personal_file, 'r', encoding='utf-8') as f:
            personal_info = json.load(f)
        logger.info(f"ä¸ªäººçŸ¥è¯†åº“æ–‡ä»¶è¯»å–æˆåŠŸï¼ŒåŒ…å« {len(personal_info.get('knowledge_bullets', []))} ä¸ªçŸ¥è¯†æ¡ç›®")
            
        # è¯»å–ä¸»çŸ¥è¯†åº“
        main_kb_file = Path("data/playbook.json")
        logger.info(f"è¯»å–ä¸»çŸ¥è¯†åº“æ–‡ä»¶: {main_kb_file}")
        
        if main_kb_file.exists():
            with open(main_kb_file, 'r', encoding='utf-8') as f:
                main_kb = json.load(f)
            logger.info(f"ä¸»çŸ¥è¯†åº“æ–‡ä»¶è¯»å–æˆåŠŸï¼Œå½“å‰åŒ…å« {len(main_kb.get('bullets', {}))} ä¸ªæ¡ç›®")
        else:
            main_kb = {"bullets": {}}
            logger.info("ä¸»çŸ¥è¯†åº“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°çš„çŸ¥è¯†åº“ç»“æ„")
            
        # ç¡®ä¿bulletsæ˜¯å­—å…¸ç±»å‹
        if not isinstance(main_kb.get("bullets"), dict):
            main_kb["bullets"] = {}
            logger.info("ç¡®ä¿bulletså­—æ®µä¸ºå­—å…¸ç±»å‹")
            
        # ç¡®ä¿sectionså­—æ®µå­˜åœ¨
        if "sections" not in main_kb:
            main_kb["sections"] = {}
            logger.info("æ·»åŠ sectionså­—æ®µåˆ°ä¸»çŸ¥è¯†åº“")
            
        # æ·»åŠ ä¸ªäººçŸ¥è¯†æ¡ç›®åˆ°ä¸»çŸ¥è¯†åº“
        logger.info(f"å¼€å§‹æ·»åŠ  {len(personal_info['knowledge_bullets'])} ä¸ªä¸ªäººçŸ¥è¯†æ¡ç›®åˆ°ä¸»çŸ¥è¯†åº“")
        
        for i, bullet in enumerate(personal_info["knowledge_bullets"], 1):
            logger.info(f"å¤„ç†çŸ¥è¯†æ¡ç›® {i}/{len(personal_info['knowledge_bullets'])}: {bullet['id']}")
            
            # åˆ›å»ºå…¼å®¹ACEæ¡†æ¶çš„bulletæ•°æ®ï¼Œç§»é™¤metadataå­—æ®µ
            ace_bullet = {
                "id": bullet["id"],
                "section": bullet["section"],
                "content": bullet["content"],
                "helpful": bullet["helpful"],
                "harmful": bullet["harmful"],
                "neutral": bullet.get("neutral", 0),
                "created_at": bullet.get("created_at", datetime.now().isoformat()),
                "updated_at": bullet.get("updated_at", datetime.now().isoformat())
            }
            logger.info(f"åˆ›å»ºACEå…¼å®¹çš„bulletæ•°æ®: {ace_bullet['id']}")
            
            # ç›´æ¥ä½¿ç”¨idä½œä¸ºé”®æ·»åŠ åˆ°å­—å…¸ä¸­
            main_kb["bullets"][bullet["id"]] = ace_bullet
            logger.info(f"æ·»åŠ bulletåˆ°ä¸»çŸ¥è¯†åº“: {bullet['id']}")
            
            # åŒæ—¶æ›´æ–°sectionså­—æ®µ
            section = bullet["section"]
            if section not in main_kb["sections"]:
                main_kb["sections"][section] = []
                logger.info(f"åˆ›å»ºæ–°çš„section: {section}")
            if bullet["id"] not in main_kb["sections"][section]:
                main_kb["sections"][section].append(bullet["id"])
                logger.info(f"æ·»åŠ bulletåˆ°section {section}: {bullet['id']}")
                
        logger.info(f"æ‰€æœ‰ä¸ªäººçŸ¥è¯†æ¡ç›®å·²æ·»åŠ åˆ°ä¸»çŸ¥è¯†åº“")
        logger.info(f"æ›´æ–°åçš„ä¸»çŸ¥è¯†åº“åŒ…å« {len(main_kb['bullets'])} ä¸ªæ¡ç›®")
        logger.info(f"æ›´æ–°åçš„sections: {list(main_kb['sections'].keys())}")
                
        # ä¿å­˜æ›´æ–°åçš„ä¸»çŸ¥è¯†åº“
        logger.info(f"ä¿å­˜æ›´æ–°åçš„ä¸»çŸ¥è¯†åº“åˆ°æ–‡ä»¶: {main_kb_file}")
        with open(main_kb_file, 'w', encoding='utf-8') as f:
            json.dump(main_kb, f, ensure_ascii=False, indent=2)
        logger.info(f"ä¸»çŸ¥è¯†åº“æ–‡ä»¶ä¿å­˜æˆåŠŸ")
            
        logger.info(f"å·²å°† {name} çš„ä¸ªäººçŸ¥è¯†åº“æ›´æ–°åˆ°ä¸»çŸ¥è¯†åº“")
        return True
        
    except Exception as e:
        logger.error(f"æ›´æ–°ä¸»çŸ¥è¯†åº“å¤±è´¥: {e}", exc_info=True)
        return False

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥"""
    return jsonify({"status": "healthy", "service": "resume_processor"})

@app.route('/api/resume/upload', methods=['POST'])
def upload_resume():
    """å¤„ç†ç®€å†ä¸Šä¼ """
    request_id = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]  # ç”Ÿæˆè¯·æ±‚ID
    logger.info(f"[{request_id}] å¼€å§‹å¤„ç†ç®€å†ä¸Šä¼ è¯·æ±‚")
    
    try:
        # è®°å½•è¯·æ±‚ä¿¡æ¯
        logger.info(f"[{request_id}] è¯·æ±‚æ¥æº: {request.remote_addr}")
        logger.info(f"[{request_id}] è¯·æ±‚å¤´: {dict(request.headers)}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if 'resume' not in request.files:
            logger.error(f"[{request_id}] é”™è¯¯: æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶")
            return jsonify({"success": False, "error": "æ²¡æœ‰ä¸Šä¼ æ–‡ä»¶"}), 400
            
        file = request.files['resume']
        if file.filename == '':
            logger.error(f"[{request_id}] é”™è¯¯: æ²¡æœ‰é€‰æ‹©æ–‡ä»¶")
            return jsonify({"success": False, "error": "æ²¡æœ‰é€‰æ‹©æ–‡ä»¶"}), 400
            
        logger.info(f"[{request_id}] ä¸Šä¼ æ–‡ä»¶å: {file.filename}")
        logger.info(f"[{request_id}] æ–‡ä»¶å¤§å°: {file.content_length} bytes")
        logger.info(f"[{request_id}] æ–‡ä»¶ç±»å‹: {file.content_type}")
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        if not allowed_file(file.filename):
            logger.error(f"[{request_id}] é”™è¯¯: ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ {file.filename}")
            return jsonify({"success": False, "error": "ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹"}), 400
            
        logger.info(f"[{request_id}] æ–‡ä»¶ç±»å‹æ£€æŸ¥é€šè¿‡")
        
        # ä¿å­˜æ–‡ä»¶
        filename = secure_filename(file.filename)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{timestamp}_{filename}"
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        
        logger.info(f"[{request_id}] ä¿å­˜æ–‡ä»¶åˆ°: {file_path}")
        file.save(file_path)
        logger.info(f"[{request_id}] æ–‡ä»¶ä¿å­˜æˆåŠŸ")
        
        # æå–æ–‡æœ¬å†…å®¹
        logger.info(f"[{request_id}] å¼€å§‹æå–æ–‡ä»¶æ–‡æœ¬å†…å®¹")
        content = extract_text_from_file(file_path)
        if not content:
            logger.error(f"[{request_id}] é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹")
            return jsonify({"success": False, "error": "æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹"}), 400
            
        logger.info(f"[{request_id}] æ–‡æœ¬æå–æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
        logger.debug(f"[{request_id}] æå–çš„æ–‡æœ¬å†…å®¹é¢„è§ˆ: {content[:200]}...")
            
        # ä½¿ç”¨AIæå–ä¿¡æ¯
        logger.info(f"[{request_id}] å¼€å§‹AIä¿¡æ¯æå–")
        extracted_info = call_ai_for_extraction(content)
        if not extracted_info:
            logger.error(f"[{request_id}] é”™è¯¯: AIä¿¡æ¯æå–å¤±è´¥")
            return jsonify({"success": False, "error": "ä¿¡æ¯æå–å¤±è´¥"}), 500
            
        logger.info(f"[{request_id}] AIä¿¡æ¯æå–æˆåŠŸ")
        logger.info(f"[{request_id}] æå–çš„ä¿¡æ¯: {extracted_info}")
            
        # è·å–å§“å
        name = extracted_info.get("name", "æœªçŸ¥ç”¨æˆ·")
        logger.info(f"[{request_id}] æå–çš„å§“å: {name}")
        
        # åˆ›å»ºä¸ªäººçŸ¥è¯†åº“
        logger.info(f"[{request_id}] å¼€å§‹åˆ›å»ºä¸ªäººçŸ¥è¯†åº“")
        personal_kb_path = create_personal_knowledge_base(name, extracted_info)
        if not personal_kb_path:
            logger.error(f"[{request_id}] é”™è¯¯: åˆ›å»ºä¸ªäººçŸ¥è¯†åº“å¤±è´¥")
            return jsonify({"success": False, "error": "åˆ›å»ºä¸ªäººçŸ¥è¯†åº“å¤±è´¥"}), 500
            
        logger.info(f"[{request_id}] ä¸ªäººçŸ¥è¯†åº“åˆ›å»ºæˆåŠŸ: {personal_kb_path}")
            
        # æ›´æ–°ä¸»çŸ¥è¯†åº“
        logger.info(f"[{request_id}] å¼€å§‹æ›´æ–°ä¸»çŸ¥è¯†åº“ï¼Œä¸ªäººçŸ¥è¯†åº“è·¯å¾„: {personal_kb_path}")
        update_result = update_main_knowledge_base(personal_kb_path, name)
        logger.info(f"[{request_id}] ä¸»çŸ¥è¯†åº“æ›´æ–°ç»“æœ: {update_result}")
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        try:
            os.remove(file_path)
            logger.info(f"[{request_id}] ä¸´æ—¶æ–‡ä»¶æ¸…ç†æˆåŠŸ: {file_path}")
        except Exception as cleanup_error:
            logger.warning(f"[{request_id}] ä¸´æ—¶æ–‡ä»¶æ¸…ç†å¤±è´¥: {cleanup_error}")
            
        logger.info(f"[{request_id}] ç®€å†ä¸Šä¼ å¤„ç†å®Œæˆ")
        return jsonify({
            "success": True,
            "analysis": extracted_info,
            "personal_kb_path": personal_kb_path,
            "message": f"ç®€å†åˆ†æå®Œæˆï¼Œå·²ä¸º {name} åˆ›å»ºä¸ªäººçŸ¥è¯†åº“"
        })
        
    except Exception as e:
        logger.error(f"[{request_id}] ç®€å†ä¸Šä¼ å¤„ç†å¤±è´¥: {e}", exc_info=True)
        return jsonify({"success": False, "error": str(e)}), 500


@app.route('/api/resume/gtv-assessment', methods=['POST'])
def gtv_assessment():
    """GTVèµ„æ ¼è¯„ä¼°"""
    request_id = datetime.now().strftime("%Y%m%d_%H%M%S_%f")[:-3]  # ç”Ÿæˆè¯·æ±‚ID
    logger.info(f"[{request_id}] å¼€å§‹GTVèµ„æ ¼è¯„ä¼°è¯·æ±‚")
    
    try:
        # è·å–è¯·æ±‚æ•°æ®
        data = request.get_json()
        if not data:
            logger.error(f"[{request_id}] é”™è¯¯: æ²¡æœ‰æä¾›è¯„ä¼°æ•°æ®")
            return jsonify({"success": False, "error": "æ²¡æœ‰æä¾›è¯„ä¼°æ•°æ®"}), 400
            
        # æå–å¿…è¦å‚æ•°
        extracted_info = data.get('extracted_info', {})
        field = data.get('field', 'digital-technology')
        name = data.get('name', '')
        email = data.get('email', '')
        
        logger.info(f"[{request_id}] è¯„ä¼°å‚æ•° - å§“å: {name}, é‚®ç®±: {email}, é¢†åŸŸ: {field}")
        logger.info(f"[{request_id}] æå–çš„ä¿¡æ¯: {extracted_info}")
        
        # ä½¿ç”¨AIè¿›è¡ŒGTVè¯„ä¼°
        logger.info(f"[{request_id}] å¼€å§‹AI GTVè¯„ä¼°")
        gtv_analysis = call_ai_for_gtv_assessment(extracted_info, field)
        
        logger.info(f"[{request_id}] GTVè¯„ä¼°å®Œæˆ")
        logger.info(f"[{request_id}] è¯„ä¼°ç»“æœé¢„è§ˆ: {safe_preview(str(gtv_analysis))}")
        
        return jsonify({
            "success": True,
            "gtvAnalysis": gtv_analysis,
            "message": f"GTVèµ„æ ¼è¯„ä¼°å®Œæˆ"
        })
        
    except Exception as e:
        logger.error(f"[{request_id}] GTVè¯„ä¼°å¤±è´¥: {e}", exc_info=True)
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/api/resume/personal/<name>', methods=['GET'])
def get_personal_kb(name):
    """è·å–ä¸ªäººçŸ¥è¯†åº“"""
    try:
        personal_file = Path(f"personal_kb/{secure_filename(name)}/personal_info.json")
        if not personal_file.exists():
            return jsonify({"success": False, "error": "ä¸ªäººçŸ¥è¯†åº“ä¸å­˜åœ¨"}), 404
            
        with open(personal_file, 'r', encoding='utf-8') as f:
            personal_info = json.load(f)
            
        return jsonify({
            "success": True,
            "personal_info": personal_info
        })
        
    except Exception as e:
        logger.error(f"è·å–ä¸ªäººçŸ¥è¯†åº“å¤±è´¥: {e}")
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/api/resume/list', methods=['GET'])
def list_personal_kbs():
    """åˆ—å‡ºæ‰€æœ‰ä¸ªäººçŸ¥è¯†åº“"""
    try:
        personal_dir = Path("personal_kb")
        if not personal_dir.exists():
            return jsonify({"success": True, "personal_kbs": []})
            
        personal_kbs = []
        for kb_dir in personal_dir.iterdir():
            if kb_dir.is_dir():
                personal_file = kb_dir / "personal_info.json"
                if personal_file.exists():
                    with open(personal_file, 'r', encoding='utf-8') as f:
                        personal_info = json.load(f)
                        personal_kbs.append({
                            "name": personal_info["name"],
                            "created_at": personal_info["created_at"],
                            "last_updated": personal_info["last_updated"],
                            "knowledge_count": len(personal_info["knowledge_bullets"])
                        })
                        
        return jsonify({
            "success": True,
            "personal_kbs": personal_kbs
        })
        
    except Exception as e:
        logger.error(f"åˆ—å‡ºä¸ªäººçŸ¥è¯†åº“å¤±è´¥: {e}")
        return jsonify({"success": False, "error": str(e)}), 500

if __name__ == '__main__':
    logger.info("ğŸš€ å¯åŠ¨ç®€å†å¤„ç†æœåŠ¡...")
    logger.info("ğŸ“¡ APIåœ°å€: http://localhost:5002")
    logger.info("ğŸ”— å¥åº·æ£€æŸ¥: http://localhost:5002/health")
    logger.info("ğŸ“„ ç®€å†ä¸Šä¼ : http://localhost:5002/api/resume/upload")
    logger.info("ğŸ“š ä¸ªäººçŸ¥è¯†åº“: http://localhost:5002/api/resume/personal/<name>")
    
    app.run(host='0.0.0.0', port=5002, debug=True)
