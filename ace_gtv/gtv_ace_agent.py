#!/usr/bin/env python3
"""
GTVç­¾è¯è¯„ä¼°çš„ACEè‡ªæˆ‘è¿›åŒ–ä»£ç†
åŸºäºAgentic Context Engineeringæ¡†æ¶å®ç°
"""

import json
import os
import sys
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from datetime import datetime
import logging

# æ·»åŠ ACEæ¡†æ¶è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'ACE-open'))

try:
    from ace import (
        Playbook, Bullet, Generator, Reflector, Curator,
        OfflineAdapter, OnlineAdapter, Sample, TaskEnvironment, EnvironmentResult,
        DummyLLMClient
    )
except ImportError:
    print("è¯·ç¡®ä¿ACE-openæ¡†æ¶å·²æ­£ç¡®å®‰è£…")
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class GTVAssessmentData:
    """GTVè¯„ä¼°æ•°æ®ç»“æ„"""
    name: str = ""
    field: str = ""
    experience: str = ""
    education: str = ""
    achievements: List[str] = field(default_factory=lambda: [])
    current_score: int = 0
    pathway: str = ""
    eligibility_criteria: Dict[str, Any] = field(default_factory=lambda: {})

class GTVTaskEnvironment(TaskEnvironment):
    """GTVç­¾è¯è¯„ä¼°ä»»åŠ¡ç¯å¢ƒ"""
    
    def __init__(self):
        self.gtv_criteria = {
            "exceptional_talent": {
                "min_score": 80,
                "requirements": ["å›½é™…è®¤å¯", "æ°å‡ºæˆå°±", "è¡Œä¸šé¢†å¯¼åŠ›"]
            },
            "exceptional_promise": {
                "min_score": 70,
                "requirements": ["åˆ›æ–°æ½œåŠ›", "æœªæ¥è´¡çŒ®", "ä¸“ä¸šå‘å±•"]
            },
            "startup_visa": {
                "min_score": 60,
                "requirements": ["åˆ›æ–°å•†ä¸šè®¡åˆ’", "èµ„é‡‘æ”¯æŒ", "å¸‚åœºæ½œåŠ›"]
            }
        }
    
    def evaluate(self, sample: Sample, generator_output) -> EnvironmentResult:
        """è¯„ä¼°GTVç”³è¯·å›ç­”çš„è´¨é‡"""
        try:
            # è§£æç”Ÿæˆå™¨çš„å›ç­”
            answer_data = json.loads(generator_output.final_answer) if isinstance(generator_output.final_answer, str) else generator_output.final_answer
            
            # è¯„ä¼°é€»è¾‘
            score = self._calculate_gtv_score(answer_data, sample)
            feedback = self._generate_feedback(score, answer_data)
            
            return EnvironmentResult(
                feedback=feedback,
                ground_truth=sample.ground_truth,
                metrics={
                    "gtv_score": score,
                    "completeness": self._assess_completeness(answer_data),
                    "relevance": self._assess_relevance(sample.question, answer_data),
                    "accuracy": self._assess_accuracy(answer_data, sample.ground_truth)
                }
            )
        except Exception as e:
            logger.error(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return EnvironmentResult(
                feedback="è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·é‡æ–°å°è¯•",
                ground_truth=sample.ground_truth,
                metrics={"error": 1.0}
            )
    
    def _calculate_gtv_score(self, answer_data: Dict, sample: Sample) -> int:
        """è®¡ç®—GTVè¯„åˆ†"""
        score = 0
        
        # åŸºäºå›ç­”å†…å®¹è¯„åˆ†
        if isinstance(answer_data, dict):
            if "field" in answer_data and answer_data["field"]:
                score += 20
            if "experience" in answer_data and answer_data["experience"]:
                score += 25
            if "education" in answer_data and answer_data["education"]:
                score += 15
            if "achievements" in answer_data and answer_data["achievements"]:
                score += 30
            if "pathway" in answer_data and answer_data["pathway"]:
                score += 10
        
        # åŸºäºé—®é¢˜ç›¸å…³æ€§è¯„åˆ†
        question_lower = sample.question.lower()
        if "exceptional talent" in question_lower and "exceptional talent" in str(answer_data).lower():
            score += 20
        elif "exceptional promise" in question_lower and "exceptional promise" in str(answer_data).lower():
            score += 20
        elif "startup" in question_lower and "startup" in str(answer_data).lower():
            score += 20
        
        return min(score, 100)
    
    def _assess_completeness(self, answer_data: Dict) -> float:
        """è¯„ä¼°å›ç­”çš„å®Œæ•´æ€§"""
        required_fields = ["field", "experience", "education", "achievements"]
        present_fields = sum(1 for field in required_fields if field in answer_data and answer_data[field])
        return present_fields / len(required_fields)
    
    def _assess_relevance(self, question: str, answer_data: Dict) -> float:
        """è¯„ä¼°å›ç­”çš„ç›¸å…³æ€§"""
        question_words = set(question.lower().split())
        answer_text = str(answer_data).lower()
        answer_words = set(answer_text.split())
        
        if not question_words:
            return 0.0
        
        common_words = question_words.intersection(answer_words)
        return len(common_words) / len(question_words)
    
    def _assess_accuracy(self, answer_data: Dict, ground_truth: Optional[str]) -> float:
        """è¯„ä¼°å›ç­”çš„å‡†ç¡®æ€§"""
        if not ground_truth:
            return 0.5  # æ²¡æœ‰æ ‡å‡†ç­”æ¡ˆæ—¶ç»™ä¸­ç­‰åˆ†æ•°
        
        # ç®€å•çš„å…³é”®è¯åŒ¹é…
        answer_text = str(answer_data).lower()
        truth_words = set(ground_truth.lower().split())
        answer_words = set(answer_text.split())
        
        if not truth_words:
            return 0.5
        
        common_words = truth_words.intersection(answer_words)
        return len(common_words) / len(truth_words)
    
    def _generate_feedback(self, score: int, answer_data: Dict) -> str:
        """ç”Ÿæˆåé¦ˆä¿¡æ¯"""
        if score >= 80:
            return f"ä¼˜ç§€ï¼æ‚¨çš„å›ç­”å¾—åˆ†{score}åˆ†ï¼Œç¬¦åˆGTV Exceptional Talentç­¾è¯è¦æ±‚ã€‚"
        elif score >= 70:
            return f"è‰¯å¥½ï¼æ‚¨çš„å›ç­”å¾—åˆ†{score}åˆ†ï¼Œç¬¦åˆGTV Exceptional Promiseç­¾è¯è¦æ±‚ã€‚"
        elif score >= 60:
            return f"åˆæ ¼ï¼æ‚¨çš„å›ç­”å¾—åˆ†{score}åˆ†ï¼Œç¬¦åˆGTV Startupç­¾è¯è¦æ±‚ã€‚"
        else:
            return f"éœ€è¦æ”¹è¿›ã€‚æ‚¨çš„å›ç­”å¾—åˆ†{score}åˆ†ï¼Œå»ºè®®åŠ å¼ºç›¸å…³ç»éªŒå’Œæˆå°±çš„æè¿°ã€‚"

class GTVACEAgent:
    """GTVç­¾è¯è¯„ä¼°çš„ACEè‡ªæˆ‘è¿›åŒ–ä»£ç†"""
    
    def __init__(self, llm_client=None):
        self.llm_client = llm_client or DummyLLMClient()
        self.playbook = self._initialize_gtv_playbook()
        self.generator = Generator(self.llm_client)
        self.reflector = Reflector(self.llm_client)
        self.curator = Curator(self.llm_client)
        self.environment = GTVTaskEnvironment()
        self.adapter = OnlineAdapter(
            playbook=self.playbook,
            generator=self.generator,
            reflector=self.reflector,
            curator=self.curator
        )
        self.conversation_history = []
        self.assessment_data = GTVAssessmentData()
    
    def _initialize_gtv_playbook(self) -> Playbook:
        """åˆå§‹åŒ–GTVä¸“ä¸šçŸ¥è¯†åº“"""
        initial_bullets = [
            Bullet(
                id="gtv_overview",
                content="GTV (Global Talent Visa) æ˜¯è‹±å›½ä¸ºå¸å¼•å…¨çƒé¡¶å°–äººæ‰è€Œè®¾ç«‹çš„ç­¾è¯ç±»åˆ«",
                section="defaults",
                metadata={"helpful": 5, "harmful": 0}
            ),
            Bullet(
                id="exceptional_talent",
                content="Exceptional Talentç­¾è¯è¦æ±‚ç”³è¯·äººåœ¨å…¶é¢†åŸŸå†…å…·æœ‰å›½é™…è®¤å¯çš„æ°å‡ºæˆå°±",
                section="guidelines",
                metadata={"helpful": 8, "harmful": 0}
            ),
            Bullet(
                id="exceptional_promise",
                content="Exceptional Promiseç­¾è¯é¢å‘å…·æœ‰åˆ›æ–°æ½œåŠ›å’Œæœªæ¥è´¡çŒ®èƒ½åŠ›çš„ä¸“ä¸šäººå£«",
                section="guidelines",
                metadata={"helpful": 7, "harmful": 0}
            ),
            Bullet(
                id="startup_visa",
                content="Startup Visaé¢å‘å…·æœ‰åˆ›æ–°å•†ä¸šè®¡åˆ’çš„åˆ›ä¸šè€…",
                section="guidelines",
                metadata={"helpful": 6, "harmful": 0}
            ),
            Bullet(
                id="assessment_criteria",
                content="è¯„ä¼°æ ‡å‡†åŒ…æ‹¬ï¼šä¸“ä¸šèƒŒæ™¯ã€å·¥ä½œç»éªŒã€æ•™è‚²èƒŒæ™¯ã€æˆå°±è®°å½•ã€æœªæ¥è´¡çŒ®æ½œåŠ›",
                section="guidelines",
                metadata={"helpful": 9, "harmful": 0}
            )
        ]
        
        playbook = Playbook()
        for bullet in initial_bullets:
            playbook.bullets.append(bullet)
        
        return playbook
    
    def process_question(self, question: str, context: str = "") -> Dict[str, Any]:
        """å¤„ç†ç”¨æˆ·é—®é¢˜å¹¶è¿”å›è¯„ä¼°ç»“æœ"""
        try:
            # åˆ›å»ºæ ·æœ¬
            sample = Sample(
                question=question,
                context=context,
                ground_truth=self._get_ground_truth(question),
                metadata={
                    "timestamp": datetime.now().isoformat(),
                    "conversation_id": len(self.conversation_history)
                }
            )
            
            # ä½¿ç”¨ACEå¤„ç†
            results = self.adapter.run([sample], self.environment)
            result = results[0] if results else None
            
            if not result:
                return self._create_error_response("å¤„ç†å¤±è´¥")
            
            # æ›´æ–°è¯„ä¼°æ•°æ®
            self._update_assessment_data(result)
            
            # è®°å½•å¯¹è¯å†å²
            self.conversation_history.append({
                "question": question,
                "answer": result.generator_output.final_answer,
                "score": result.environment_result.metrics.get("gtv_score", 0),
                "timestamp": datetime.now().isoformat()
            })
            
            return {
                "success": True,
                "answer": result.generator_output.final_answer,
                "reasoning": result.generator_output.reasoning,
                "score": result.environment_result.metrics.get("gtv_score", 0),
                "feedback": result.environment_result.feedback,
                "assessment_data": self.assessment_data.__dict__,
                "playbook_stats": self.playbook.stats(),
                "evolution_insights": self._extract_evolution_insights(result)
            }
            
        except Exception as e:
            logger.error(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}")
            return self._create_error_response(f"å¤„ç†å¤±è´¥: {str(e)}")
    
    def _get_ground_truth(self, question: str) -> Optional[str]:
        """æ ¹æ®é—®é¢˜è·å–æ ‡å‡†ç­”æ¡ˆ"""
        question_lower = question.lower()
        
        if "exceptional talent" in question_lower:
            return "Exceptional Talentç­¾è¯è¦æ±‚ç”³è¯·äººåœ¨å…¶é¢†åŸŸå†…å…·æœ‰å›½é™…è®¤å¯çš„æ°å‡ºæˆå°±ï¼ŒåŒ…æ‹¬è·å¥–è®°å½•ã€ä¸“åˆ©ã€å‘è¡¨è®ºæ–‡ç­‰ã€‚"
        elif "exceptional promise" in question_lower:
            return "Exceptional Promiseç­¾è¯é¢å‘å…·æœ‰åˆ›æ–°æ½œåŠ›å’Œæœªæ¥è´¡çŒ®èƒ½åŠ›çš„ä¸“ä¸šäººå£«ï¼Œéœ€è¦å±•ç¤ºæœªæ¥5-10å¹´çš„å‘å±•è®¡åˆ’ã€‚"
        elif "startup" in question_lower:
            return "Startup Visaé¢å‘å…·æœ‰åˆ›æ–°å•†ä¸šè®¡åˆ’çš„åˆ›ä¸šè€…ï¼Œéœ€è¦è·å¾—è®¤å¯æœºæ„çš„èƒŒä¹¦ã€‚"
        
        return None
    
    def _update_assessment_data(self, result) -> None:
        """æ›´æ–°è¯„ä¼°æ•°æ®"""
        try:
            answer_data = json.loads(result.generator_output.final_answer) if isinstance(result.generator_output.final_answer, str) else result.generator_output.final_answer
            
            if isinstance(answer_data, dict):
                self.assessment_data.name = answer_data.get("name", self.assessment_data.name)
                self.assessment_data.field = answer_data.get("field", self.assessment_data.field)
                self.assessment_data.experience = answer_data.get("experience", self.assessment_data.experience)
                self.assessment_data.education = answer_data.get("education", self.assessment_data.education)
                self.assessment_data.achievements = answer_data.get("achievements", self.assessment_data.achievements)
                self.assessment_data.pathway = answer_data.get("pathway", self.assessment_data.pathway)
                self.assessment_data.current_score = result.environment_result.metrics.get("gtv_score", 0)
        except Exception as e:
            logger.warning(f"æ›´æ–°è¯„ä¼°æ•°æ®æ—¶å‡ºé”™: {e}")
    
    def _extract_evolution_insights(self, result) -> Dict[str, Any]:
        """æå–è‡ªæˆ‘è¿›åŒ–çš„æ´å¯Ÿ"""
        return {
            "new_bullets_added": len(result.curator_output.delta.operations) if hasattr(result.curator_output, 'delta') else 0,
            "reflection_insights": result.reflection.key_insight if hasattr(result.reflection, 'key_insight') else "",
            "playbook_evolution": {
                "total_bullets": self.playbook.stats()["total_bullets"],
                "helpful_bullets": self.playbook.stats()["helpful_bullets"],
                "harmful_bullets": self.playbook.stats()["harmful_bullets"]
            }
        }
    
    def _create_error_response(self, message: str) -> Dict[str, Any]:
        """åˆ›å»ºé”™è¯¯å“åº”"""
        return {
            "success": False,
            "error": message,
            "answer": "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é—®é¢˜ã€‚è¯·ç¨åé‡è¯•ã€‚",
            "score": 0,
            "feedback": message
        }
    
    def get_playbook_status(self) -> Dict[str, Any]:
        """è·å–çŸ¥è¯†åº“çŠ¶æ€"""
        return {
            "stats": self.playbook.stats(),
            "playbook_content": self.playbook.as_prompt(),
            "conversation_count": len(self.conversation_history)
        }
    
    def reset_assessment(self) -> None:
        """é‡ç½®è¯„ä¼°"""
        self.assessment_data = GTVAssessmentData()
        self.conversation_history = []
        logger.info("è¯„ä¼°å·²é‡ç½®")

def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•ACEä»£ç†"""
    print("ğŸš€ å¯åŠ¨GTV ACEè‡ªæˆ‘è¿›åŒ–ä»£ç†...")
    
    # åˆ›å»ºä»£ç†
    agent = GTVACEAgent()
    
    # æµ‹è¯•é—®é¢˜
    test_questions = [
        "æˆ‘æƒ³ç”³è¯·GTV Exceptional Talentç­¾è¯ï¼Œéœ€è¦ä»€ä¹ˆæ¡ä»¶ï¼Ÿ",
        "æˆ‘çš„èƒŒæ™¯æ˜¯AIç ”å‘ï¼Œæœ‰5å¹´ç»éªŒï¼Œèƒ½ç”³è¯·å“ªç§GTVç­¾è¯ï¼Ÿ",
        "GTVç­¾è¯çš„è¯„ä¼°æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ"
    ]
    
    for question in test_questions:
        print(f"\nâ“ é—®é¢˜: {question}")
        result = agent.process_question(question)
        
        if result["success"]:
            print(f"âœ… å›ç­”: {result['answer']}")
            print(f"ğŸ“Š è¯„åˆ†: {result['score']}/100")
            print(f"ğŸ’¡ åé¦ˆ: {result['feedback']}")
        else:
            print(f"âŒ é”™è¯¯: {result['error']}")
    
    # æ˜¾ç¤ºçŸ¥è¯†åº“çŠ¶æ€
    print(f"\nğŸ“š çŸ¥è¯†åº“çŠ¶æ€:")
    status = agent.get_playbook_status()
    print(f"æ€»æ¡ç›®æ•°: {status['stats']['total_bullets']}")
    print(f"æœ‰ç”¨æ¡ç›®: {status['stats']['helpful_bullets']}")
    print(f"æœ‰å®³æ¡ç›®: {status['stats']['harmful_bullets']}")

if __name__ == "__main__":
    main()
