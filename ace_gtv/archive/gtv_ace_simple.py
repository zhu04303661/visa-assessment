#!/usr/bin/env python3
"""
GTVç­¾è¯è¯„ä¼°çš„ACEè‡ªæˆ‘è¿›åŒ–ä»£ç† - ç®€åŒ–ç‰ˆæœ¬
"""

import json
import os
import sys
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, field
from datetime import datetime
import logging

# æ·»åŠ ACEæ¡†æ¶è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'ACE-open'))

try:
    from ace import (
        Playbook, Bullet, Generator, Reflector, Curator,
        OfflineAdapter, OnlineAdapter, Sample, TaskEnvironment, EnvironmentResult,
        DummyLLMClient
    )
    print("âœ… ACEæ¡†æ¶å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ ACEæ¡†æ¶å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class GTVAssessmentData:
    """GTVè¯„ä¼°æ•°æ®ç»“æ„"""
    name: str = ""
    field: str = ""
    experience: str = ""
    education: str = ""
    achievements: list = field(default_factory=list)
    current_score: int = 0
    pathway: str = ""
    eligibility_criteria: dict = field(default_factory=dict)

class GTVTaskEnvironment(TaskEnvironment):
    """GTVç­¾è¯è¯„ä¼°ä»»åŠ¡ç¯å¢ƒ"""
    
    def __init__(self):
        self.gtv_criteria = {
            "exceptional_talent": {
                "min_score": 80,
                "requirements": ["å›½é™…è®¤å¯", "æ°å‡ºæˆå°±", "è¡Œä¸šé¢†å¯¼åŠ›"]
            },
            "exceptional_promise": {
                "min_score": 70,
                "requirements": ["åˆ›æ–°æ½œåŠ›", "æœªæ¥è´¡çŒ®", "ä¸“ä¸šå‘å±•"]
            },
            "startup_visa": {
                "min_score": 60,
                "requirements": ["åˆ›æ–°å•†ä¸šè®¡åˆ’", "èµ„é‡‘æ”¯æŒ", "å¸‚åœºæ½œåŠ›"]
            }
        }
    
    def evaluate(self, sample: Sample, generator_output) -> EnvironmentResult:
        """è¯„ä¼°GTVç”³è¯·å›ç­”çš„è´¨é‡"""
        try:
            # ç®€åŒ–çš„è¯„ä¼°é€»è¾‘
            score = 75  # é»˜è®¤åˆ†æ•°
            feedback = f"åŸºäºæ‚¨çš„å›ç­”ï¼ŒGTVè¯„ä¼°å¾—åˆ†ä¸º{score}åˆ†ã€‚"
            
            return EnvironmentResult(
                feedback=feedback,
                ground_truth=sample.ground_truth,
                metrics={
                    "gtv_score": score,
                    "completeness": 0.8,
                    "relevance": 0.9,
                    "accuracy": 0.85
                }
            )
        except Exception as e:
            logger.error(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return EnvironmentResult(
                feedback="è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·é‡æ–°å°è¯•",
                ground_truth=sample.ground_truth,
                metrics={"error": 1.0}
            )

class GTVACEAgent:
    """GTVç­¾è¯è¯„ä¼°çš„ACEè‡ªæˆ‘è¿›åŒ–ä»£ç†"""
    
    def __init__(self, llm_client=None):
        self.llm_client = llm_client or DummyLLMClient()
        self.playbook = self._initialize_gtv_playbook()
        self.generator = Generator(self.llm_client)
        self.reflector = Reflector(self.llm_client)
        self.curator = Curator(self.llm_client)
        self.environment = GTVTaskEnvironment()
        self.adapter = OnlineAdapter(
            playbook=self.playbook,
            generator=self.generator,
            reflector=self.reflector,
            curator=self.curator
        )
        self.conversation_history = []
        self.assessment_data = GTVAssessmentData()
    
    def _initialize_gtv_playbook(self) -> Playbook:
        """åˆå§‹åŒ–GTVä¸“ä¸šçŸ¥è¯†åº“"""
        playbook = Playbook()
        
        # æ·»åŠ åˆå§‹çŸ¥è¯†æ¡ç›®
        initial_bullets = [
            {
                "id": "gtv_overview",
                "content": "GTV (Global Talent Visa) æ˜¯è‹±å›½ä¸ºå¸å¼•å…¨çƒé¡¶å°–äººæ‰è€Œè®¾ç«‹çš„ç­¾è¯ç±»åˆ«",
                "section": "defaults",
                "helpful": 5,
                "harmful": 0
            },
            {
                "id": "exceptional_talent",
                "content": "Exceptional Talentç­¾è¯è¦æ±‚ç”³è¯·äººåœ¨å…¶é¢†åŸŸå†…å…·æœ‰å›½é™…è®¤å¯çš„æ°å‡ºæˆå°±",
                "section": "guidelines",
                "helpful": 8,
                "harmful": 0
            },
            {
                "id": "exceptional_promise",
                "content": "Exceptional Promiseç­¾è¯é¢å‘å…·æœ‰åˆ›æ–°æ½œåŠ›å’Œæœªæ¥è´¡çŒ®èƒ½åŠ›çš„ä¸“ä¸šäººå£«",
                "section": "guidelines",
                "helpful": 7,
                "harmful": 0
            },
            {
                "id": "startup_visa",
                "content": "Startup Visaé¢å‘å…·æœ‰åˆ›æ–°å•†ä¸šè®¡åˆ’çš„åˆ›ä¸šè€…",
                "section": "guidelines",
                "helpful": 6,
                "harmful": 0
            },
            {
                "id": "assessment_criteria",
                "content": "è¯„ä¼°æ ‡å‡†åŒ…æ‹¬ï¼šä¸“ä¸šèƒŒæ™¯ã€å·¥ä½œç»éªŒã€æ•™è‚²èƒŒæ™¯ã€æˆå°±è®°å½•ã€æœªæ¥è´¡çŒ®æ½œåŠ›",
                "section": "guidelines",
                "helpful": 9,
                "harmful": 0
            }
        ]
        
        for bullet_data in initial_bullets:
            bullet = Bullet(
                id=bullet_data["id"],
                content=bullet_data["content"],
                section=bullet_data["section"],
                helpful=bullet_data["helpful"],
                harmful=bullet_data["harmful"]
            )
            playbook.add_bullet(bullet)
        
        return playbook
    
    def process_question(self, question: str, context: str = "") -> dict:
        """å¤„ç†ç”¨æˆ·é—®é¢˜å¹¶è¿”å›è¯„ä¼°ç»“æœ"""
        try:
            # åˆ›å»ºæ ·æœ¬
            sample = Sample(
                question=question,
                context=context,
                ground_truth=self._get_ground_truth(question),
                metadata={
                    "timestamp": datetime.now().isoformat(),
                    "conversation_id": len(self.conversation_history)
                }
            )
            
            # ä½¿ç”¨ACEå¤„ç†
            results = self.adapter.run([sample], self.environment)
            result = results[0] if results else None
            
            if not result:
                return self._create_error_response("å¤„ç†å¤±è´¥")
            
            # æ›´æ–°è¯„ä¼°æ•°æ®
            self._update_assessment_data(result)
            
            # è®°å½•å¯¹è¯å†å²
            self.conversation_history.append({
                "question": question,
                "answer": result.generator_output.final_answer,
                "score": result.environment_result.metrics.get("gtv_score", 0),
                "timestamp": datetime.now().isoformat()
            })
            
            return {
                "success": True,
                "answer": result.generator_output.final_answer,
                "reasoning": result.generator_output.reasoning,
                "score": result.environment_result.metrics.get("gtv_score", 0),
                "feedback": result.environment_result.feedback,
                "assessment_data": self.assessment_data.__dict__,
                "playbook_stats": self.playbook.stats(),
                "evolution_insights": self._extract_evolution_insights(result)
            }
            
        except Exception as e:
            logger.error(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}")
            return self._create_error_response(f"å¤„ç†å¤±è´¥: {str(e)}")
    
    def _get_ground_truth(self, question: str) -> Optional[str]:
        """æ ¹æ®é—®é¢˜è·å–æ ‡å‡†ç­”æ¡ˆ"""
        question_lower = question.lower()
        
        if "exceptional talent" in question_lower:
            return "Exceptional Talentç­¾è¯è¦æ±‚ç”³è¯·äººåœ¨å…¶é¢†åŸŸå†…å…·æœ‰å›½é™…è®¤å¯çš„æ°å‡ºæˆå°±ï¼ŒåŒ…æ‹¬è·å¥–è®°å½•ã€ä¸“åˆ©ã€å‘è¡¨è®ºæ–‡ç­‰ã€‚"
        elif "exceptional promise" in question_lower:
            return "Exceptional Promiseç­¾è¯é¢å‘å…·æœ‰åˆ›æ–°æ½œåŠ›å’Œæœªæ¥è´¡çŒ®èƒ½åŠ›çš„ä¸“ä¸šäººå£«ï¼Œéœ€è¦å±•ç¤ºæœªæ¥5-10å¹´çš„å‘å±•è®¡åˆ’ã€‚"
        elif "startup" in question_lower:
            return "Startup Visaé¢å‘å…·æœ‰åˆ›æ–°å•†ä¸šè®¡åˆ’çš„åˆ›ä¸šè€…ï¼Œéœ€è¦è·å¾—è®¤å¯æœºæ„çš„èƒŒä¹¦ã€‚"
        
        return None
    
    def _update_assessment_data(self, result) -> None:
        """æ›´æ–°è¯„ä¼°æ•°æ®"""
        try:
            answer_data = result.generator_output.final_answer
            if isinstance(answer_data, str):
                try:
                    answer_data = json.loads(answer_data)
                except json.JSONDecodeError:
                    answer_data = {"text": answer_data}
            
            if isinstance(answer_data, dict):
                self.assessment_data.name = answer_data.get("name", self.assessment_data.name)
                self.assessment_data.field = answer_data.get("field", self.assessment_data.field)
                self.assessment_data.experience = answer_data.get("experience", self.assessment_data.experience)
                self.assessment_data.education = answer_data.get("education", self.assessment_data.education)
                self.assessment_data.achievements = answer_data.get("achievements", self.assessment_data.achievements)
                self.assessment_data.pathway = answer_data.get("pathway", self.assessment_data.pathway)
                self.assessment_data.current_score = result.environment_result.metrics.get("gtv_score", 0)
        except Exception as e:
            logger.warning(f"æ›´æ–°è¯„ä¼°æ•°æ®æ—¶å‡ºé”™: {e}")
    
    def _extract_evolution_insights(self, result) -> dict:
        """æå–è‡ªæˆ‘è¿›åŒ–çš„æ´å¯Ÿ"""
        return {
            "new_bullets_added": len(result.curator_output.delta.operations) if hasattr(result.curator_output, 'delta') else 0,
            "reflection_insights": result.reflection.key_insight if hasattr(result.reflection, 'key_insight') else "",
            "playbook_evolution": {
                "total_bullets": self.playbook.stats()["total_bullets"],
                "helpful_bullets": self.playbook.stats()["helpful_bullets"],
                "harmful_bullets": self.playbook.stats()["harmful_bullets"]
            }
        }
    
    def _create_error_response(self, message: str) -> dict:
        """åˆ›å»ºé”™è¯¯å“åº”"""
        return {
            "success": False,
            "error": message,
            "answer": "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é—®é¢˜ã€‚è¯·ç¨åé‡è¯•ã€‚",
            "score": 0,
            "feedback": message
        }
    
    def get_playbook_status(self) -> dict:
        """è·å–çŸ¥è¯†åº“çŠ¶æ€"""
        return {
            "stats": self.playbook.stats(),
            "playbook_content": self.playbook.as_prompt(),
            "conversation_count": len(self.conversation_history)
        }
    
    def reset_assessment(self) -> None:
        """é‡ç½®è¯„ä¼°"""
        self.assessment_data = GTVAssessmentData()
        self.conversation_history = []
        logger.info("è¯„ä¼°å·²é‡ç½®")

def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•ACEä»£ç†"""
    print("ğŸš€ å¯åŠ¨GTV ACEè‡ªæˆ‘è¿›åŒ–ä»£ç†...")
    
    # åˆ›å»ºä»£ç†
    agent = GTVACEAgent()
    
    # æµ‹è¯•é—®é¢˜
    test_questions = [
        "æˆ‘æƒ³ç”³è¯·GTV Exceptional Talentç­¾è¯ï¼Œéœ€è¦ä»€ä¹ˆæ¡ä»¶ï¼Ÿ",
        "æˆ‘çš„èƒŒæ™¯æ˜¯AIç ”å‘ï¼Œæœ‰5å¹´ç»éªŒï¼Œèƒ½ç”³è¯·å“ªç§GTVç­¾è¯ï¼Ÿ",
        "GTVç­¾è¯çš„è¯„ä¼°æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ"
    ]
    
    for question in test_questions:
        print(f"\nâ“ é—®é¢˜: {question}")
        result = agent.process_question(question)
        
        if result["success"]:
            print(f"âœ… å›ç­”: {result['answer']}")
            print(f"ğŸ“Š è¯„åˆ†: {result['score']}/100")
            print(f"ğŸ’¡ åé¦ˆ: {result['feedback']}")
        else:
            print(f"âŒ é”™è¯¯: {result['error']}")
    
    # æ˜¾ç¤ºçŸ¥è¯†åº“çŠ¶æ€
    print(f"\nğŸ“š çŸ¥è¯†åº“çŠ¶æ€:")
    status = agent.get_playbook_status()
    print(f"æ€»æ¡ç›®æ•°: {status['stats']['total_bullets']}")
    print(f"æœ‰ç”¨æ¡ç›®: {status['stats']['helpful_bullets']}")
    print(f"æœ‰å®³æ¡ç›®: {status['stats']['harmful_bullets']}")

if __name__ == "__main__":
    main()
