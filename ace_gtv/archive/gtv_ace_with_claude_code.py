#!/usr/bin/env python3
"""
GTVç­¾è¯è¯„ä¼°çš„ACEè‡ªæˆ‘è¿›åŒ–ä»£ç† - é›†æˆClaude Codeå‘½ä»¤ç‰ˆæœ¬
ä½¿ç”¨Claude Codeå‘½ä»¤æ›¿ä»£ä¼ ç»Ÿåˆ†æè¯„ä¼°è¿‡ç¨‹
"""

import json
import os
import sys
import subprocess
import tempfile
from pathlib import Path
from typing import Dict, Optional, Any, List
from datetime import datetime
import logging

# æ·»åŠ ACEæ¡†æ¶è·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'ACE-open'))

try:
    from ace import (
        Playbook, Generator, Reflector, Curator,
        OnlineAdapter, Sample, TaskEnvironment, EnvironmentResult,
        DummyLLMClient
    )
    print("âœ… ACEæ¡†æ¶å¯¼å…¥æˆåŠŸ")
except ImportError as e:
    print(f"âŒ ACEæ¡†æ¶å¯¼å…¥å¤±è´¥: {e}")
    sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ClaudeCodeEvaluator:
    """Claude Codeå‘½ä»¤è¯„ä¼°å™¨"""
    
    def __init__(self, claude_code_path: str = "claude-code"):
        self.claude_code_path = claude_code_path
        self.temp_dir = tempfile.mkdtemp(prefix="gtv_claude_")
        
    def analyze_with_claude_code(self, question: str, context: str = "") -> Dict[str, Any]:
        """ä½¿ç”¨Claude Codeåˆ†æé—®é¢˜"""
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
            question_file = os.path.join(self.temp_dir, "question.txt")
            with open(question_file, 'w', encoding='utf-8') as f:
                f.write(f"é—®é¢˜: {question}\n\nä¸Šä¸‹æ–‡: {context}")
            
            # ä½¿ç”¨Claude Codeåˆ†æ
            cmd = f"{self.claude_code_path} ask 'åŸºäºGTVç­¾è¯è¯„ä¼°æ ‡å‡†ï¼Œåˆ†æä»¥ä¸‹é—®é¢˜å¹¶æä¾›ä¸“ä¸šå»ºè®®: {question}'"
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0:
                return {
                    "success": True,
                    "answer": result.stdout,
                    "reasoning": f"Claude Codeåˆ†æ: {result.stdout}",
                    "claude_code_output": result.stdout
                }
            else:
                return {
                    "success": False,
                    "error": f"Claude Codeæ‰§è¡Œå¤±è´¥: {result.stderr}",
                    "answer": "Claude Codeåˆ†æä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ"
                }
                
        except Exception as e:
            logger.error(f"Claude Codeåˆ†æå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "answer": "Claude Codeåˆ†æä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤åˆ†æ"
            }
    
    def evaluate_with_claude_code(self, answer: str, question: str) -> Dict[str, Any]:
        """ä½¿ç”¨Claude Codeè¯„ä¼°å›ç­”è´¨é‡"""
        try:
            # åˆ›å»ºè¯„ä¼°è„šæœ¬
            eval_script = f"""
# GTVè¯„ä¼°è„šæœ¬
answer = '''{answer}'''
question = '''{question}'''

def evaluate_gtv_response(answer, question):
    score = 70  # åŸºç¡€åˆ†æ•°
    
    # åŸºäºå…³é”®è¯è¯„ä¼°
    keywords = ['GTV', 'ç­¾è¯', 'exceptional', 'talent', 'promise', 'startup']
    keyword_count = sum(1 for keyword in keywords if keyword.lower() in answer.lower())
    score += min(keyword_count * 5, 20)
    
    # åŸºäºé•¿åº¦è¯„ä¼°
    if len(answer) > 100:
        score += 10
    
    return min(score, 100)

print(json.dumps({{
    "score": evaluate_gtv_response(answer, question),
    "feedback": "åŸºäºClaude Codeè¯„ä¼°çš„åé¦ˆ",
    "completeness": 0.8,
    "relevance": 0.9,
    "accuracy": 0.85
}}, ensure_ascii=False))
"""
            
            script_file = os.path.join(self.temp_dir, "evaluate.py")
            with open(script_file, 'w', encoding='utf-8') as f:
                f.write(eval_script)
            
            # ä½¿ç”¨Claude Codeæ‰§è¡Œè¯„ä¼°
            cmd = f"{self.claude_code_path} run --script {script_file}"
            result = subprocess.run(cmd, shell=True, capture_output=True, text=True, timeout=30)
            
            if result.returncode == 0:
                try:
                    evaluation = json.loads(result.stdout)
                    return evaluation
                except json.JSONDecodeError:
                    return {
                        "score": 75,
                        "feedback": result.stdout,
                        "completeness": 0.8,
                        "relevance": 0.9,
                        "accuracy": 0.85
                    }
            else:
                return self._create_fallback_evaluation()
                
        except Exception as e:
            logger.error(f"Claude Codeè¯„ä¼°å¤±è´¥: {e}")
            return self._create_fallback_evaluation()
    
    def _create_fallback_evaluation(self) -> Dict[str, Any]:
        """åˆ›å»ºå¤‡ç”¨è¯„ä¼°ç»“æœ"""
        return {
            "score": 70,
            "feedback": "Claude Codeè¯„ä¼°ä¸å¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤è¯„ä¼°",
            "completeness": 0.7,
            "relevance": 0.8,
            "accuracy": 0.75
        }
    
    def cleanup(self):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            import shutil
            shutil.rmtree(self.temp_dir)
        except Exception as e:
            logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

class GTVAssessmentData:
    """GTVè¯„ä¼°æ•°æ®ç»“æ„"""
    def __init__(self):
        self.name = ""
        self.field = ""
        self.experience = ""
        self.education = ""
        self.achievements = []
        self.current_score = 0
        self.pathway = ""
        self.eligibility_criteria = {}

class GTVTaskEnvironment(TaskEnvironment):
    """GTVç­¾è¯è¯„ä¼°ä»»åŠ¡ç¯å¢ƒ - é›†æˆClaude Code"""
    
    def __init__(self, claude_code_evaluator: ClaudeCodeEvaluator = None):
        self.claude_code_evaluator = claude_code_evaluator or ClaudeCodeEvaluator()
        self.gtv_criteria = {
            "exceptional_talent": {
                "min_score": 80,
                "requirements": ["å›½é™…è®¤å¯", "æ°å‡ºæˆå°±", "è¡Œä¸šé¢†å¯¼åŠ›"]
            },
            "exceptional_promise": {
                "min_score": 70,
                "requirements": ["åˆ›æ–°æ½œåŠ›", "æœªæ¥è´¡çŒ®", "ä¸“ä¸šå‘å±•"]
            },
            "startup_visa": {
                "min_score": 60,
                "requirements": ["åˆ›æ–°å•†ä¸šè®¡åˆ’", "èµ„é‡‘æ”¯æŒ", "å¸‚åœºæ½œåŠ›"]
            }
        }
    
    def evaluate(self, sample: Sample, generator_output) -> EnvironmentResult:
        """è¯„ä¼°GTVç”³è¯·å›ç­”çš„è´¨é‡ - ä½¿ç”¨Claude Code"""
        try:
            # ä½¿ç”¨Claude Codeè¿›è¡Œè¯„ä¼°
            evaluation = self.claude_code_evaluator.evaluate_with_claude_code(
                generator_output.final_answer, 
                sample.question
            )
            
            return EnvironmentResult(
                feedback=evaluation.get("feedback", "Claude Codeè¯„ä¼°å®Œæˆ"),
                ground_truth=sample.ground_truth,
                metrics={
                    "gtv_score": evaluation.get("score", 70),
                    "completeness": evaluation.get("completeness", 0.8),
                    "relevance": evaluation.get("relevance", 0.9),
                    "accuracy": evaluation.get("accuracy", 0.85)
                }
            )
        except Exception as e:
            logger.error(f"Claude Codeè¯„ä¼°è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return EnvironmentResult(
                feedback="Claude Codeè¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·é‡æ–°å°è¯•",
                ground_truth=sample.ground_truth,
                metrics={"error": 1.0}
            )

class GTVACEAgentWithClaudeCode:
    """GTVç­¾è¯è¯„ä¼°çš„ACEè‡ªæˆ‘è¿›åŒ–ä»£ç† - é›†æˆClaude Codeç‰ˆæœ¬"""
    
    def __init__(self, llm_client=None, claude_code_path: str = "claude-code", default_mode: str = "ace"):
        if llm_client is None:
            # åˆ›å»ºé…ç½®äº†é»˜è®¤å“åº”çš„DummyLLMClient
            llm_client = self._create_configured_dummy_client()
        self.llm_client = llm_client
        self.claude_code_evaluator = ClaudeCodeEvaluator(claude_code_path)
        self.default_mode = default_mode  # "ace" æˆ– "claude_code"
        self.playbook = self._initialize_gtv_playbook()
        self.generator = Generator(self.llm_client)
        self.reflector = Reflector(self.llm_client)
        self.curator = Curator(self.llm_client)
        self.environment = GTVTaskEnvironment(self.claude_code_evaluator)
        self.adapter = OnlineAdapter(
            playbook=self.playbook,
            generator=self.generator,
            reflector=self.reflector,
            curator=self.curator
        )
        self.conversation_history = []
        self.assessment_data = GTVAssessmentData()

        # è®¾ç½®æ•°æ®ç›®å½•ç”¨äºä¿å­˜æ–‡ä»¶
        self.data_dir = Path(__file__).parent / "data"
        self.data_dir.mkdir(exist_ok=True)
    
    def _create_configured_dummy_client(self) -> DummyLLMClient:
        """åˆ›å»ºé…ç½®äº†GTVç›¸å…³å“åº”çš„DummyLLMClient"""
        client = DummyLLMClient()
        
        # ä¸ºGeneratoré…ç½®å“åº”
        client.queue(json.dumps({
            "reasoning": "åŸºäºGTVç­¾è¯è¯„ä¼°æ ‡å‡†åˆ†æç”¨æˆ·é—®é¢˜",
            "bullet_ids": ["gtv_overview", "exceptional_talent"],
            "final_answer": "GTVç­¾è¯æ˜¯è‹±å›½ä¸ºå¸å¼•å…¨çƒé¡¶å°–äººæ‰è€Œè®¾ç«‹çš„ç­¾è¯ç±»åˆ«ã€‚ä¸»è¦åˆ†ä¸ºä¸‰ç§ï¼š1) Exceptional Talent - éœ€è¦å›½é™…è®¤å¯çš„æ°å‡ºæˆå°±ï¼›2) Exceptional Promise - éœ€è¦å±•ç¤ºæœªæ¥æ½œåŠ›ï¼›3) Startup Visa - éœ€è¦åˆ›æ–°å•†ä¸šè®¡åˆ’ã€‚"
        }))
        
        # ä¸ºReflectoré…ç½®å“åº”
        client.queue(json.dumps({
            "reasoning": "åˆ†æGTVè¯„ä¼°ç»“æœçš„å‡†ç¡®æ€§",
            "error_identification": "æ— é‡å¤§é”™è¯¯",
            "root_cause_analysis": "å›ç­”æ¶µç›–äº†GTVç­¾è¯çš„åŸºæœ¬ä¿¡æ¯",
            "correct_approach": "ç»§ç»­æä¾›å‡†ç¡®çš„GTVç­¾è¯ä¿¡æ¯",
            "key_insight": "ç”¨æˆ·éœ€è¦äº†è§£ä¸åŒGTVç­¾è¯ç±»å‹çš„å…·ä½“è¦æ±‚",
            "bullet_tags": []
        }))
        
        # ä¸ºCuratoré…ç½®å“åº”
        client.queue(json.dumps({
            "reasoning": "æ ¹æ®ç”¨æˆ·é—®é¢˜æ›´æ–°çŸ¥è¯†åº“",
            "operations": [
                {
                    "type": "ADD",
                    "section": "guidelines",
                    "content": "GTVç­¾è¯ç”³è¯·éœ€è¦æ ¹æ®ä¸ªäººèƒŒæ™¯é€‰æ‹©åˆé€‚çš„ç­¾è¯ç±»å‹",
                    "metadata": {"helpful": 1}
                }
            ]
        }))
        
        return client
    
    def _initialize_gtv_playbook(self) -> Playbook:
        """åˆå§‹åŒ–GTVä¸“ä¸šçŸ¥è¯†åº“"""
        playbook = Playbook()
        
        # æ·»åŠ åˆå§‹çŸ¥è¯†æ¡ç›®
        initial_bullets = [
            {
                "id": "gtv_overview",
                "content": "GTV (Global Talent Visa) æ˜¯è‹±å›½ä¸ºå¸å¼•å…¨çƒé¡¶å°–äººæ‰è€Œè®¾ç«‹çš„ç­¾è¯ç±»åˆ«",
                "section": "defaults",
                "helpful": 5,
                "harmful": 0
            },
            {
                "id": "claude_code_integration",
                "content": "ä½¿ç”¨Claude Codeå‘½ä»¤è¿›è¡Œä»£ç åˆ†æå’Œè¯„ä¼°ï¼Œæä¾›æ›´æ™ºèƒ½çš„è¯„ä¼°ç»“æœ",
                "section": "guidelines",
                "helpful": 8,
                "harmful": 0
            },
            {
                "id": "exceptional_talent",
                "content": "Exceptional Talentç­¾è¯è¦æ±‚ç”³è¯·äººåœ¨å…¶é¢†åŸŸå†…å…·æœ‰å›½é™…è®¤å¯çš„æ°å‡ºæˆå°±",
                "section": "guidelines",
                "helpful": 8,
                "harmful": 0
            },
            {
                "id": "exceptional_promise",
                "content": "Exceptional Promiseç­¾è¯é¢å‘å…·æœ‰åˆ›æ–°æ½œåŠ›å’Œæœªæ¥è´¡çŒ®èƒ½åŠ›çš„ä¸“ä¸šäººå£«",
                "section": "guidelines",
                "helpful": 7,
                "harmful": 0
            },
            {
                "id": "startup_visa",
                "content": "Startup Visaé¢å‘å…·æœ‰åˆ›æ–°å•†ä¸šè®¡åˆ’çš„åˆ›ä¸šè€…",
                "section": "guidelines",
                "helpful": 6,
                "harmful": 0
            },
            {
                "id": "assessment_criteria",
                "content": "è¯„ä¼°æ ‡å‡†åŒ…æ‹¬ï¼šä¸“ä¸šèƒŒæ™¯ã€å·¥ä½œç»éªŒã€æ•™è‚²èƒŒæ™¯ã€æˆå°±è®°å½•ã€æœªæ¥è´¡çŒ®æ½œåŠ›",
                "section": "guidelines",
                "helpful": 9,
                "harmful": 0
            }
        ]
        
        for bullet_data in initial_bullets:
            playbook.add_bullet(
                section=bullet_data["section"],
                content=bullet_data["content"],
                bullet_id=bullet_data["id"],
                metadata={
                    "helpful": bullet_data["helpful"],
                    "harmful": bullet_data["harmful"]
                }
            )
        
        return playbook
    
    def process_question(self, question: str, context: str = "", use_claude_code: bool = None) -> dict:
        """å¤„ç†ç”¨æˆ·é—®é¢˜å¹¶è¿”å›è¯„ä¼°ç»“æœ"""
        try:
            # å†³å®šä½¿ç”¨å“ªç§æ¨¡å¼
            if use_claude_code is None:
                use_claude_code = (self.default_mode == "claude_code")
            
            if use_claude_code:
                # ä½¿ç”¨Claude Codeè¿›è¡Œåˆ†æ
                claude_code_result = self.claude_code_evaluator.analyze_with_claude_code(question, context)
                
                if claude_code_result["success"]:
                    # å¦‚æœClaude Codeåˆ†ææˆåŠŸï¼Œç›´æ¥è¿”å›ç»“æœ
                    self._update_assessment_data_from_claude_code(claude_code_result)
                    
                    # è®°å½•å¯¹è¯å†å²
                    self.conversation_history.append({
                        "question": question,
                        "answer": claude_code_result["answer"],
                        "score": 75,  # Claude Codeé»˜è®¤åˆ†æ•°
                        "timestamp": datetime.now().isoformat(),
                        "method": "claude_code"
                    })
                    
                    return {
                        "success": True,
                        "answer": claude_code_result["answer"],
                        "reasoning": claude_code_result.get("reasoning", ""),
                        "score": 75,
                        "feedback": "åŸºäºClaude Codeåˆ†æçš„è¯„ä¼°ç»“æœ",
                        "assessment_data": self.assessment_data.__dict__,
                        "playbook_stats": self.playbook.stats(),
                        "method": "claude_code",
                        "claude_code_output": claude_code_result.get("claude_code_output", "")
                    }
                else:
                    # å¦‚æœClaude Codeå¤±è´¥ï¼Œå›é€€åˆ°ACEæ–¹æ³•
                    logger.warning("Claude Codeåˆ†æå¤±è´¥ï¼Œå›é€€åˆ°ACEæ¨¡å¼")
                    return self._fallback_to_ace(question, context)
            else:
                # é»˜è®¤ä½¿ç”¨ACEæ¨¡å¼
                return self._process_with_ace(question, context)
            
        except Exception as e:
            logger.error(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {e}")
            return self._create_error_response(f"å¤„ç†å¤±è´¥: {str(e)}")
    
    def _process_with_ace(self, question: str, context: str) -> dict:
        """ä½¿ç”¨ACEæ¨¡å¼å¤„ç†é—®é¢˜ï¼ˆé»˜è®¤æ¨¡å¼ï¼‰"""
        try:
            # åˆ›å»ºæ ·æœ¬
            sample = Sample(
                question=question,
                ground_truth="",  # GTVè¯„ä¼°æ²¡æœ‰æ ‡å‡†ç­”æ¡ˆ
                context=context
            )
            
            # ä½¿ç”¨ACEé€‚é…å™¨å¤„ç†
            results = self.adapter.run([sample], self.environment)
            if not results:
                return self._create_error_response("ACEå¤„ç†å¤±è´¥ï¼šæ²¡æœ‰è¿”å›ç»“æœ")
            
            step_result = results[0]
            
            # æ›´æ–°è¯„ä¼°æ•°æ®
            self._update_assessment_data_from_ace(step_result)
            
            # è®°å½•å¯¹è¯å†å²
            self.conversation_history.append({
                "question": question,
                "answer": step_result.generator_output.final_answer,
                "score": step_result.environment_result.metrics.get("gtv_score", 70),
                "timestamp": datetime.now().isoformat(),
                "method": "ace"
            })
            
            return {
                "success": True,
                "answer": step_result.generator_output.final_answer,
                "reasoning": step_result.generator_output.reasoning,
                "score": step_result.environment_result.metrics.get("gtv_score", 70),
                "feedback": step_result.environment_result.feedback,
                "assessment_data": self.assessment_data.__dict__,
                "playbook_stats": self.playbook.stats(),
                "method": "ace",
                "metrics": step_result.environment_result.metrics
            }
            
        except Exception as e:
            logger.error(f"ACEå¤„ç†è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return self._create_error_response(f"ACEå¤„ç†å¤±è´¥: {str(e)}")
    
    def _fallback_to_ace(self, question: str, context: str) -> dict:
        """å›é€€åˆ°ä¼ ç»ŸACEæ–¹æ³•"""
        try:
            # åˆ›å»ºæ ·æœ¬
            sample = Sample(
                question=question,
                context=context,
                ground_truth=self._get_ground_truth(question),
                metadata={
                    "timestamp": datetime.now().isoformat(),
                    "conversation_id": len(self.conversation_history)
                }
            )
            
            # ä½¿ç”¨ACEå¤„ç†
            results = self.adapter.run([sample], self.environment)
            result = results[0] if results else None
            
            if not result:
                return self._create_error_response("ACEå¤„ç†å¤±è´¥")
            
            # æ›´æ–°è¯„ä¼°æ•°æ®
            self._update_assessment_data(result)
            
            # è®°å½•å¯¹è¯å†å²
            self.conversation_history.append({
                "question": question,
                "answer": result.generator_output.final_answer,
                "score": result.environment_result.metrics.get("gtv_score", 0),
                "timestamp": datetime.now().isoformat(),
                "method": "ace_fallback"
            })
            
            return {
                "success": True,
                "answer": result.generator_output.final_answer,
                "reasoning": result.generator_output.reasoning,
                "score": result.environment_result.metrics.get("gtv_score", 0),
                "feedback": result.environment_result.feedback,
                "assessment_data": self.assessment_data.__dict__,
                "playbook_stats": self.playbook.stats(),
                "evolution_insights": self._extract_evolution_insights(result),
                "method": "ace_fallback"
            }
            
        except Exception as e:
            logger.error(f"ACEå›é€€å¤„ç†å¤±è´¥: {e}")
            return self._create_error_response(f"ACEå›é€€å¤„ç†å¤±è´¥: {str(e)}")
    
    def _update_assessment_data_from_ace(self, step_result) -> None:
        """ä»ACEç»“æœæ›´æ–°è¯„ä¼°æ•°æ®"""
        try:
            # ä»ACEç»“æœä¸­æå–è¯„ä¼°æ•°æ®
            metrics = step_result.environment_result.metrics
            self.assessment_data.current_score = metrics.get("gtv_score", 70)
            self.assessment_data.completeness = metrics.get("completeness", 0.8)
            self.assessment_data.accuracy = metrics.get("accuracy", 0.85)
            
            # æ ¹æ®åˆ†æ•°ç¡®å®šç­¾è¯è·¯å¾„
            if self.assessment_data.current_score >= 80:
                self.assessment_data.pathway = "exceptional_talent"
            elif self.assessment_data.current_score >= 70:
                self.assessment_data.pathway = "exceptional_promise"
            else:
                self.assessment_data.pathway = "startup_visa"
                
        except Exception as e:
            logger.error(f"æ›´æ–°ACEè¯„ä¼°æ•°æ®æ—¶å‡ºé”™: {e}")
    
    def _update_assessment_data_from_claude_code(self, claude_code_result: Dict[str, Any]) -> None:
        """ä»Claude Codeç»“æœæ›´æ–°è¯„ä¼°æ•°æ®"""
        try:
            answer = claude_code_result.get("answer", "")
            # ç®€åŒ–çš„æ•°æ®æå–é€»è¾‘
            if "exceptional talent" in answer.lower():
                self.assessment_data.pathway = "exceptional_talent"
            elif "exceptional promise" in answer.lower():
                self.assessment_data.pathway = "exceptional_promise"
            elif "startup" in answer.lower():
                self.assessment_data.pathway = "startup_visa"
            
            self.assessment_data.current_score = 75  # Claude Codeé»˜è®¤åˆ†æ•°
        except Exception as e:
            logger.warning(f"ä»Claude Codeç»“æœæ›´æ–°è¯„ä¼°æ•°æ®æ—¶å‡ºé”™: {e}")
    
    def _get_ground_truth(self, question: str) -> Optional[str]:
        """æ ¹æ®é—®é¢˜è·å–æ ‡å‡†ç­”æ¡ˆ"""
        question_lower = question.lower()
        
        if "exceptional talent" in question_lower:
            return "Exceptional Talentç­¾è¯è¦æ±‚ç”³è¯·äººåœ¨å…¶é¢†åŸŸå†…å…·æœ‰å›½é™…è®¤å¯çš„æ°å‡ºæˆå°±ï¼ŒåŒ…æ‹¬è·å¥–è®°å½•ã€ä¸“åˆ©ã€å‘è¡¨è®ºæ–‡ç­‰ã€‚"
        elif "exceptional promise" in question_lower:
            return "Exceptional Promiseç­¾è¯é¢å‘å…·æœ‰åˆ›æ–°æ½œåŠ›å’Œæœªæ¥è´¡çŒ®èƒ½åŠ›çš„ä¸“ä¸šäººå£«ï¼Œéœ€è¦å±•ç¤ºæœªæ¥5-10å¹´çš„å‘å±•è®¡åˆ’ã€‚"
        elif "startup" in question_lower:
            return "Startup Visaé¢å‘å…·æœ‰åˆ›æ–°å•†ä¸šè®¡åˆ’çš„åˆ›ä¸šè€…ï¼Œéœ€è¦è·å¾—è®¤å¯æœºæ„çš„èƒŒä¹¦ã€‚"
        
        return None
    
    def _update_assessment_data(self, result) -> None:
        """æ›´æ–°è¯„ä¼°æ•°æ®"""
        try:
            answer_data = result.generator_output.final_answer
            if isinstance(answer_data, str):
                try:
                    answer_data = json.loads(answer_data)
                except json.JSONDecodeError:
                    answer_data = {"text": answer_data}
            
            if isinstance(answer_data, dict):
                self.assessment_data.name = answer_data.get("name", self.assessment_data.name)
                self.assessment_data.field = answer_data.get("field", self.assessment_data.field)
                self.assessment_data.experience = answer_data.get("experience", self.assessment_data.experience)
                self.assessment_data.education = answer_data.get("education", self.assessment_data.education)
                self.assessment_data.achievements = answer_data.get("achievements", self.assessment_data.achievements)
                self.assessment_data.pathway = answer_data.get("pathway", self.assessment_data.pathway)
                self.assessment_data.current_score = result.environment_result.metrics.get("gtv_score", 0)
        except Exception as e:
            logger.warning(f"æ›´æ–°è¯„ä¼°æ•°æ®æ—¶å‡ºé”™: {e}")
    
    def _extract_evolution_insights(self, result) -> dict:
        """æå–è‡ªæˆ‘è¿›åŒ–çš„æ´å¯Ÿ"""
        return {
            "new_bullets_added": len(result.curator_output.delta.operations) if hasattr(result.curator_output, 'delta') else 0,
            "reflection_insights": result.reflection.key_insight if hasattr(result.reflection, 'key_insight') else "",
            "playbook_evolution": {
                "total_bullets": self.playbook.stats()["total_bullets"],
                "helpful_bullets": self.playbook.stats()["helpful_bullets"],
                "harmful_bullets": self.playbook.stats()["harmful_bullets"]
            }
        }
    
    def _create_error_response(self, message: str) -> dict:
        """åˆ›å»ºé”™è¯¯å“åº”"""
        return {
            "success": False,
            "error": message,
            "answer": "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°äº†é—®é¢˜ã€‚è¯·ç¨åé‡è¯•ã€‚",
            "score": 0,
            "feedback": message
        }
    
    def get_playbook_status(self) -> dict:
        """è·å–çŸ¥è¯†åº“çŠ¶æ€"""
        return {
            "stats": self.playbook.stats(),
            "playbook_content": self.playbook.as_prompt(),
            "conversation_count": len(self.conversation_history),
            "default_mode": self.default_mode
        }
    
    def set_default_mode(self, mode: str) -> None:
        """è®¾ç½®é»˜è®¤æ¨¡å¼"""
        if mode in ["ace", "claude_code"]:
            self.default_mode = mode
            logger.info(f"é»˜è®¤æ¨¡å¼å·²è®¾ç½®ä¸º: {mode}")
        else:
            logger.error(f"æ— æ•ˆçš„æ¨¡å¼: {mode}ï¼Œæ”¯æŒçš„æ¨¡å¼: ace, claude_code")
    
    def get_current_mode(self) -> str:
        """è·å–å½“å‰é»˜è®¤æ¨¡å¼"""
        return self.default_mode
    
    def reset_assessment(self) -> None:
        """é‡ç½®è¯„ä¼°"""
        self.assessment_data = GTVAssessmentData()
        self.conversation_history = []
        logger.info("è¯„ä¼°å·²é‡ç½®")

    def _save_playbook(self) -> None:
        """ä¿å­˜çŸ¥è¯†åº“"""
        try:
            playbook_file = self.data_dir / "playbook.json"
            with open(playbook_file, 'w', encoding='utf-8') as f:
                json.dump(self.playbook.to_dict(), f, ensure_ascii=False, indent=2)
            logger.info("çŸ¥è¯†åº“å·²ä¿å­˜")
        except Exception as e:
            logger.error(f"ä¿å­˜çŸ¥è¯†åº“å¤±è´¥: {e}")

    def _save_conversation_history(self) -> None:
        """ä¿å­˜å¯¹è¯å†å²"""
        try:
            history_file = self.data_dir / "conversation_history.json"
            with open(history_file, 'w', encoding='utf-8') as f:
                json.dump(self.conversation_history, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜å¯¹è¯å†å²å¤±è´¥: {e}")

    def _save_assessment_data(self) -> None:
        """ä¿å­˜è¯„ä¼°æ•°æ®"""
        try:
            assessment_file = self.data_dir / "assessment_data.json"
            with open(assessment_file, 'w', encoding='utf-8') as f:
                json.dump(self.assessment_data.__dict__, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"ä¿å­˜è¯„ä¼°æ•°æ®å¤±è´¥: {e}")

    def get_all_bullets(self) -> List[Dict]:
        """è·å–æ‰€æœ‰çŸ¥è¯†æ¡ç›®"""
        bullets = []
        for bullet in self.playbook.bullets():
            bullets.append({
                "id": bullet.id,
                "section": bullet.section,
                "content": bullet.content,
                "helpful": bullet.helpful,
                "harmful": bullet.harmful,
                "neutral": bullet.neutral,
                "created_at": bullet.created_at,
                "updated_at": bullet.updated_at
            })
        return bullets

    def add_bullet_manual(self, section: str, content: str, bullet_id: str = None) -> Dict:
        """æ‰‹åŠ¨æ·»åŠ çŸ¥è¯†æ¡ç›®"""
        try:
            bullet = self.playbook.add_bullet(
                section=section,
                content=content,
                bullet_id=bullet_id
            )
            self._save_playbook()
            return {
                "success": True,
                "bullet": {
                    "id": bullet.id,
                    "section": bullet.section,
                    "content": bullet.content,
                    "helpful": bullet.helpful,
                    "harmful": bullet.harmful,
                    "neutral": bullet.neutral
                }
            }
        except Exception as e:
            return {"success": False, "error": str(e)}

    def update_bullet_manual(self, bullet_id: str, content: str = None, section: str = None) -> Dict:
        """æ‰‹åŠ¨æ›´æ–°çŸ¥è¯†æ¡ç›®"""
        try:
            bullet = self.playbook.get_bullet(bullet_id)
            if not bullet:
                return {"success": False, "error": "çŸ¥è¯†æ¡ç›®ä¸å­˜åœ¨"}

            if content:
                bullet.content = content
            if section:
                bullet.section = section

            bullet.updated_at = datetime.now().isoformat()
            self._save_playbook()

            return {
                "success": True,
                "bullet": {
                    "id": bullet.id,
                    "section": bullet.section,
                    "content": bullet.content,
                    "helpful": bullet.helpful,
                    "harmful": bullet.harmful,
                    "neutral": bullet.neutral
                }
            }
        except Exception as e:
            return {"success": False, "error": str(e)}

    def delete_bullet_manual(self, bullet_id: str) -> Dict:
        """æ‰‹åŠ¨åˆ é™¤çŸ¥è¯†æ¡ç›®"""
        try:
            bullet = self.playbook.get_bullet(bullet_id)
            if not bullet:
                return {"success": False, "error": "çŸ¥è¯†æ¡ç›®ä¸å­˜åœ¨"}

            self.playbook.remove_bullet(bullet_id)
            self._save_playbook()

            return {"success": True, "message": "çŸ¥è¯†æ¡ç›®å·²åˆ é™¤"}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def reset_playbook(self) -> Dict:
        """é‡ç½®çŸ¥è¯†åº“"""
        try:
            self.playbook = self._initialize_gtv_playbook()
            self.conversation_history = []
            self.assessment_data = GTVAssessmentData()

            self._save_playbook()
            self._save_conversation_history()
            self._save_assessment_data()

            return {"success": True, "message": "çŸ¥è¯†åº“å·²é‡ç½®"}
        except Exception as e:
            return {"success": False, "error": str(e)}

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        self.claude_code_evaluator.cleanup()

def main():
    """ä¸»å‡½æ•°ï¼Œç”¨äºæµ‹è¯•Claude Codeé›†æˆçš„ACEä»£ç†"""
    print("ğŸš€ å¯åŠ¨GTV ACEè‡ªæˆ‘è¿›åŒ–ä»£ç† (é»˜è®¤ACEæ¨¡å¼)...")
    
    # åˆ›å»ºä»£ç†ï¼Œé»˜è®¤ä½¿ç”¨ACEæ¨¡å¼
    agent = GTVACEAgentWithClaudeCode(default_mode="ace")
    
    # æµ‹è¯•é—®é¢˜
    test_questions = [
        "æˆ‘æƒ³ç”³è¯·GTV Exceptional Talentç­¾è¯ï¼Œéœ€è¦ä»€ä¹ˆæ¡ä»¶ï¼Ÿ",
        "æˆ‘çš„èƒŒæ™¯æ˜¯AIç ”å‘ï¼Œæœ‰5å¹´ç»éªŒï¼Œèƒ½ç”³è¯·å“ªç§GTVç­¾è¯ï¼Ÿ",
        "GTVç­¾è¯çš„è¯„ä¼°æ ‡å‡†æ˜¯ä»€ä¹ˆï¼Ÿ"
    ]
    
    print(f"\nğŸ“ æµ‹è¯•é—®é¢˜å¤„ç† (å½“å‰é»˜è®¤æ¨¡å¼: {agent.get_current_mode()})...")
    
    # æµ‹è¯•ACEæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
    print("\n=== ä½¿ç”¨ACEæ¨¡å¼ï¼ˆé»˜è®¤ï¼‰ ===")
    for i, question in enumerate(test_questions[:2], 1):
        print(f"\n--- é—®é¢˜ {i} ---")
        print(f"é—®é¢˜: {question}")
        
        result = agent.process_question(question, use_claude_code=False)
        
        if result["success"]:
            print(f"âœ… å›ç­”: {result['answer']}")
            print(f"ğŸ“Š è¯„åˆ†: {result['score']}/100")
            print(f"ğŸ’¡ åé¦ˆ: {result['feedback']}")
            print(f"ğŸ”§ æ–¹æ³•: {result.get('method', 'unknown')}")
        else:
            print(f"âŒ é”™è¯¯: {result['error']}")
    
    # æµ‹è¯•Claude Codeæ¨¡å¼
    print("\n=== ä½¿ç”¨Claude Codeæ¨¡å¼ ===")
    for i, question in enumerate(test_questions[2:], 1):
        print(f"\n--- é—®é¢˜ {i} ---")
        print(f"é—®é¢˜: {question}")
        
        result = agent.process_question(question, use_claude_code=True)
        
        if result["success"]:
            print(f"âœ… å›ç­”: {result['answer']}")
            print(f"ğŸ“Š è¯„åˆ†: {result['score']}/100")
            print(f"ğŸ’¡ åé¦ˆ: {result['feedback']}")
            print(f"ğŸ”§ æ–¹æ³•: {result.get('method', 'unknown')}")
            if result.get('claude_code_output'):
                print(f"ğŸ¤– Claude Codeè¾“å‡º: {result['claude_code_output'][:100]}...")
        else:
            print(f"âŒ é”™è¯¯: {result['error']}")
    
    # æ˜¾ç¤ºçŸ¥è¯†åº“çŠ¶æ€
    print("\nğŸ“š çŸ¥è¯†åº“çŠ¶æ€:")
    status = agent.get_playbook_status()
    print(f"æ€»æ¡ç›®æ•°: {status['stats']['total_bullets']}")
    print(f"æœ‰ç”¨æ¡ç›®: {status['stats']['helpful_bullets']}")
    print(f"æœ‰å®³æ¡ç›®: {status['stats']['harmful_bullets']}")
    print(f"é»˜è®¤æ¨¡å¼: {status['default_mode']}")
    print(f"å¯¹è¯æ¬¡æ•°: {status['conversation_count']}")
    
    # æ¸…ç†èµ„æº
    agent.cleanup()

if __name__ == "__main__":
    main()

