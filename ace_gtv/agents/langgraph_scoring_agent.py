#!/usr/bin/env python3
"""
GTV LangGraph è¯„åˆ†Agent - çŸ¥è¯†åº“é©±åŠ¨çš„å¤šè½®åˆ†æ
æ”¯æŒï¼š
1. çŸ¥è¯†åº“åŠ è½½å’Œç®¡ç†
2. å¤šè½®LLMè°ƒç”¨è¿›è¡Œæ·±åº¦åˆ†æ
3. åŸºäºçŸ¥è¯†åº“çš„å®æ—¶æ›´æ–°
4. è¯¦ç»†çš„è¯„åˆ†æ¨ç†è¿‡ç¨‹è®°å½•
"""

import json
import logging
import os
from typing import Any, Dict, List, Optional, TypedDict
from datetime import datetime
from enum import Enum

try:
    from langgraph.graph import StateGraph, START, END
    from langgraph.prebuilt import create_react_agent
    HAS_LANGGRAPH = True
except ImportError:
    HAS_LANGGRAPH = False
    logging.warning("âš ï¸ LangGraph not installed, using fallback mode")

try:
    from langchain_openai import ChatOpenAI
    from langchain.tools import tool, StructuredTool
    HAS_LANGCHAIN = True
except ImportError:
    HAS_LANGCHAIN = False
    logging.warning("âš ï¸ LangChain not installed")

# ============================================================================
# æ—¥å¿—é…ç½®
# ============================================================================

from utils.logger_config import setup_module_logger

logger = setup_module_logger("scoring_agent", os.getenv("LOG_LEVEL", "INFO"))

# ============================================================================
# çŠ¶æ€å®šä¹‰
# ============================================================================

class KnowledgeBaseState(TypedDict):
    """çŸ¥è¯†åº“çŠ¶æ€"""
    rules: Dict[str, Any]  # æ‰€æœ‰çŸ¥è¯†åº“è§„åˆ™
    rule_index: Dict[str, List[str]]  # è§„åˆ™ç´¢å¼•ï¼ˆæŒ‰ç»´åº¦å’Œåˆ†ç±»ï¼‰

class ScoringAgentState(TypedDict):
    """è¯„åˆ†AgentçŠ¶æ€"""
    # åŸºæœ¬ä¿¡æ¯
    applicant_data: Dict[str, Any]
    current_score: float
    evaluation_stage: str  # current_stage
    
    # çŸ¥è¯†åº“çŠ¶æ€
    knowledge_base: KnowledgeBaseState
    relevant_rules: List[Dict[str, Any]]
    
    # å¤šè½®äº¤äº’
    conversation_history: List[Dict[str, str]]  # LLMäº¤äº’å†å²
    llm_calls: List[Dict[str, Any]]  # LLMè°ƒç”¨è®°å½•
    
    # åˆ†æç»“æœ
    criteria_analysis: Dict[str, Any]  # æ ‡å‡†åˆ†æç»“æœ
    evidence_assessment: Dict[str, Any]  # è¯æ®è¯„ä¼°
    recommendations: List[str]  # æ”¹è¿›å»ºè®®
    
    # æœ€ç»ˆæŠ¥å‘Š
    final_score: float
    final_reasoning: str
    execution_time: float

# ============================================================================
# çŸ¥è¯†åº“ç®¡ç†å™¨
# ============================================================================

class KnowledgeBaseManager:
    """çŸ¥è¯†åº“ç®¡ç†å™¨ - åŠ è½½å’Œç®¡ç†GTVè¯„ä¼°è§„åˆ™"""
    
    def __init__(self, kb_dir: str = "./public"):
        """åˆå§‹åŒ–çŸ¥è¯†åº“ç®¡ç†å™¨"""
        self.kb_dir = kb_dir
        self.rules = {}
        self.rule_index = {}
        self._load_all_rules()
    
    def _load_all_rules(self):
        """åŠ è½½æ‰€æœ‰çŸ¥è¯†åº“æ–‡ä»¶"""
        kb_files = [
            "kb-gtv-assessment-rules.json",
            "kb-checklist-rules.json",
            "kb-checklist-detailed-rules.json",
            "kb-actual-scoring-items.json",
            "kb-init-rules.json"
        ]
        
        for filename in kb_files:
            filepath = os.path.join(self.kb_dir, filename)
            if os.path.exists(filepath):
                try:
                    with open(filepath, 'r', encoding='utf-8') as f:
                        rules = json.load(f)
                        if isinstance(rules, list):
                            for rule in rules:
                                rule_id = rule.get('id', rule.get('title', f'rule_{len(self.rules)}'))
                                self.rules[rule_id] = rule
                                self._index_rule(rule)
                    logger.info(f"âœ… åŠ è½½çŸ¥è¯†åº“æ–‡ä»¶: {filename} ({len(rules)} æ¡è§„åˆ™)")
                except Exception as e:
                    logger.error(f"âŒ åŠ è½½ {filename} å¤±è´¥: {e}")
        
        logger.info(f"ğŸ“š çŸ¥è¯†åº“åŠ è½½å®Œæˆï¼Œæ€»å…± {len(self.rules)} æ¡è§„åˆ™")
    
    def _index_rule(self, rule: Dict[str, Any]):
        """ä¸ºè§„åˆ™åˆ›å»ºç´¢å¼•"""
        dimension = rule.get('dimension', 'general')
        category = rule.get('category', 'general')
        
        # æŒ‰ç»´åº¦ç´¢å¼•
        if dimension not in self.rule_index:
            self.rule_index[dimension] = []
        rule_id = rule.get('id', rule.get('title'))
        if rule_id not in self.rule_index[dimension]:
            self.rule_index[dimension].append(rule_id)
        
        # æŒ‰åˆ†ç±»ç´¢å¼•
        if category not in self.rule_index:
            self.rule_index[category] = []
        if rule_id not in self.rule_index[category]:
            self.rule_index[category].append(rule_id)
    
    def search_rules(self, dimension: Optional[str] = None, 
                    category: Optional[str] = None,
                    keywords: Optional[List[str]] = None) -> List[Dict[str, Any]]:
        """æœç´¢ç›¸å…³è§„åˆ™"""
        result = []
        
        for rule_id, rule in self.rules.items():
            # ç»´åº¦åŒ¹é…
            if dimension and rule.get('dimension') != dimension:
                continue
            
            # åˆ†ç±»åŒ¹é…
            if category and rule.get('category') != category:
                continue
            
            # å…³é”®è¯åŒ¹é…
            if keywords:
                rule_text = f"{rule.get('title', '')} {rule.get('content', '')}".lower()
                if not any(kw.lower() in rule_text for kw in keywords):
                    continue
            
            result.append(rule)
        
        return result
    
    def get_state(self) -> KnowledgeBaseState:
        """è·å–çŸ¥è¯†åº“çŠ¶æ€"""
        return {
            "rules": self.rules,
            "rule_index": self.rule_index
        }

# ============================================================================
# LangGraph è¯„åˆ†Agent
# ============================================================================

class LangGraphScoringAgent:
    """åŸºäºLangGraphçš„å¤šè½®äº¤äº’è¯„åˆ†Agent"""
    
    def __init__(self, llm=None, kb_manager: Optional[KnowledgeBaseManager] = None):
        """åˆå§‹åŒ–Agent"""
        self.llm = llm or self._init_llm()
        self.kb_manager = kb_manager or KnowledgeBaseManager()
        self.tools = self._create_tools()
        
        if HAS_LANGGRAPH:
            self.graph = self._build_langgraph()
        else:
            logger.warning("âš ï¸ LangGraph æœªå®‰è£…ï¼Œä½¿ç”¨ç®€åŒ–æ¨¡å¼")
        
        logger.info("âœ… LangGraphè¯„åˆ†Agentåˆå§‹åŒ–å®Œæˆ")
    
    def _init_llm(self):
        """åˆå§‹åŒ–LLM"""
        if not HAS_LANGCHAIN:
            logger.warning("âš ï¸ LangChainæœªå®‰è£…")
            return None
        
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            logger.warning("âš ï¸ OPENAI_API_KEYæœªè®¾ç½®")
            return None
        
        return ChatOpenAI(
            api_key=api_key,
            model="gpt-4-turbo-preview",
            temperature=0.7
        )
    
    def _create_tools(self) -> List:
        """åˆ›å»ºLLMå¯ç”¨çš„å·¥å…·"""
        tools = []
        
        # å·¥å…·1: æœç´¢çŸ¥è¯†åº“è§„åˆ™
        @tool
        def search_knowledge_base(dimension: str = None, 
                                  category: str = None,
                                  keywords: List[str] = None) -> str:
            """æœç´¢çŸ¥è¯†åº“ä¸­çš„ç›¸å…³è§„åˆ™"""
            rules = self.kb_manager.search_rules(dimension, category, keywords)
            return json.dumps([
                {
                    "title": r.get("title"),
                    "content": r.get("content"),
                    "dimension": r.get("dimension"),
                    "category": r.get("category")
                } for r in rules
            ], ensure_ascii=False, indent=2)
        
        tools.append(search_knowledge_base)
        
        # å·¥å…·2: è·å–å®Œæ•´è§„åˆ™å†…å®¹
        @tool
        def get_rule_details(rule_id: str) -> str:
            """è·å–å…·ä½“è§„åˆ™çš„è¯¦ç»†å†…å®¹"""
            rule = self.kb_manager.rules.get(rule_id)
            if rule:
                return json.dumps(rule, ensure_ascii=False, indent=2)
            return f"è§„åˆ™ {rule_id} ä¸å­˜åœ¨"
        
        tools.append(get_rule_details)
        
        # å·¥å…·3: è·å–ç»´åº¦ç›¸å…³çš„æ‰€æœ‰è§„åˆ™
        @tool
        def get_dimension_rules(dimension: str) -> str:
            """è·å–ç‰¹å®šç»´åº¦çš„æ‰€æœ‰è§„åˆ™"""
            rule_ids = self.kb_manager.rule_index.get(dimension, [])
            rules = [self.kb_manager.rules[rid] for rid in rule_ids if rid in self.kb_manager.rules]
            return json.dumps({
                "dimension": dimension,
                "rule_count": len(rules),
                "rules": [
                    {
                        "title": r.get("title"),
                        "category": r.get("category"),
                        "summary": r.get("content", "")[:200] + "..."
                    } for r in rules
                ]
            }, ensure_ascii=False, indent=2)
        
        tools.append(get_dimension_rules)
        
        return tools
    
    def _build_langgraph(self) -> StateGraph:
        """æ„å»ºLangGraphå·¥ä½œæµ"""
        if not HAS_LANGGRAPH:
            logger.warning("âš ï¸ LangGraphä¸å¯ç”¨")
            return None
        
        graph = StateGraph(ScoringAgentState)
        
        # å®šä¹‰èŠ‚ç‚¹
        graph.add_node("load_kb", self._load_kb_node)
        graph.add_node("search_relevant_rules", self._search_rules_node)
        graph.add_node("analyze_criteria", self._analyze_criteria_node)
        graph.add_node("assess_evidence", self._assess_evidence_node)
        graph.add_node("refine_analysis", self._refine_analysis_node)
        graph.add_node("generate_recommendations", self._generate_recommendations_node)
        graph.add_node("calculate_final_score", self._calculate_score_node)
        
        # å®šä¹‰è¾¹
        graph.add_edge(START, "load_kb")
        graph.add_edge("load_kb", "search_relevant_rules")
        graph.add_edge("search_relevant_rules", "analyze_criteria")
        graph.add_edge("analyze_criteria", "assess_evidence")
        graph.add_edge("assess_evidence", "refine_analysis")
        graph.add_edge("refine_analysis", "generate_recommendations")
        graph.add_edge("generate_recommendations", "calculate_final_score")
        graph.add_edge("calculate_final_score", END)
        
        return graph.compile()
    
    def _load_kb_node(self, state: ScoringAgentState) -> ScoringAgentState:
        """åŠ è½½çŸ¥è¯†åº“èŠ‚ç‚¹"""
        logger.info("ğŸ“š å¼€å§‹åŠ è½½çŸ¥è¯†åº“...")
        state["knowledge_base"] = self.kb_manager.get_state()
        state["evaluation_stage"] = "knowledge_loading"
        return state
    
    def _search_rules_node(self, state: ScoringAgentState) -> ScoringAgentState:
        """æœç´¢ç›¸å…³è§„åˆ™èŠ‚ç‚¹"""
        logger.info("ğŸ” æœç´¢ç›¸å…³è§„åˆ™...")
        
        # æ ¹æ®ç”³è¯·äººæ•°æ®æœç´¢ç›¸å…³è§„åˆ™
        applicant_data = state.get("applicant_data", {})
        keywords = []
        
        # æå–å…³é”®è¯
        if applicant_data.get("field"):
            keywords.append(applicant_data["field"])
        if applicant_data.get("position"):
            keywords.append(applicant_data["position"])
        
        # æœç´¢è§„åˆ™
        relevant_rules = self.kb_manager.search_rules(
            keywords=keywords if keywords else None
        )
        
        state["relevant_rules"] = relevant_rules
        state["evaluation_stage"] = "rules_search"
        
        logger.info(f"âœ… æ‰¾åˆ° {len(relevant_rules)} æ¡ç›¸å…³è§„åˆ™")
        
        return state
    
    def _analyze_criteria_node(self, state: ScoringAgentState) -> ScoringAgentState:
        """åˆ†ææ ‡å‡†èŠ‚ç‚¹ - ç¬¬ä¸€è½®LLMè°ƒç”¨"""
        logger.info("ğŸ“‹ ç¬¬ä¸€è½®åˆ†æï¼šæ ‡å‡†è¯„ä¼°...")
        
        if not self.llm:
            logger.warning("âš ï¸ LLMä¸å¯ç”¨ï¼Œè·³è¿‡æ­¤é˜¶æ®µ")
            return state
        
        # æ„å»ºæç¤º
        relevant_rules_text = self._format_rules_for_llm(state["relevant_rules"])
        applicant_text = json.dumps(state.get("applicant_data", {}), ensure_ascii=False, indent=2)
        
        prompt = f"""
æ‚¨æ˜¯GTVï¼ˆè‹±å›½å…¨çƒäººæ‰ç­¾è¯ï¼‰è¯„ä¼°ä¸“å®¶ã€‚
æ ¹æ®ä»¥ä¸‹çŸ¥è¯†åº“è§„åˆ™å’Œç”³è¯·äººä¿¡æ¯ï¼Œè¿›è¡Œæ ‡å‡†ç¬¦åˆæ€§åˆ†æã€‚

ã€çŸ¥è¯†åº“è§„åˆ™ã€‘
{relevant_rules_text}

ã€ç”³è¯·äººä¿¡æ¯ã€‘
{applicant_text}

è¯·è¿›è¡Œä»¥ä¸‹åˆ†æï¼š
1. ç”³è¯·äººæ˜¯å¦æ»¡è¶³å¼ºåˆ¶è¦æ±‚(MC)ï¼Ÿå…·ä½“ç†ç”±æ˜¯ä»€ä¹ˆï¼Ÿ
2. ç”³è¯·äººå¯èƒ½æ»¡è¶³å“ªäº›å¯é€‰è¦æ±‚(OC)ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
3. åœ¨è¿™äº›æ ‡å‡†ä¸­ï¼Œç”³è¯·äººæœ€å¼ºçš„æ–¹é¢æ˜¯ä»€ä¹ˆï¼Ÿ

è¯·ç”¨JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{{
  "mc_status": "æ»¡è¶³/ä¸æ»¡è¶³",
  "mc_reasoning": "å…·ä½“ç†ç”±",
  "potential_oc": ["OC1", "OC2", ...],
  "oc_reasoning": {{"OC1": "ç†ç”±", ...}},
  "strengths": ["ä¼˜åŠ¿1", "ä¼˜åŠ¿2", ...],
  "confidence": 0.0-1.0
}}
"""
        
        try:
            response = self.llm.invoke(prompt)
            criteria_analysis = json.loads(response.content)
            state["criteria_analysis"] = criteria_analysis
            
            # è®°å½•LLMè°ƒç”¨
            state["llm_calls"].append({
                "stage": "analyze_criteria",
                "timestamp": datetime.now().isoformat(),
                "prompt_length": len(prompt),
                "response_length": len(response.content)
            })
            
            # è®°å½•å¯¹è¯å†å²
            state["conversation_history"].append({
                "role": "assistant",
                "content": f"å®Œæˆæ ‡å‡†åˆ†æ: {criteria_analysis.get('mc_status', 'N/A')}"
            })
            
            logger.info(f"âœ… æ ‡å‡†åˆ†æå®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ æ ‡å‡†åˆ†æå¤±è´¥: {e}")
            state["criteria_analysis"] = {"error": str(e)}
        
        state["evaluation_stage"] = "criteria_analysis"
        return state
    
    def _assess_evidence_node(self, state: ScoringAgentState) -> ScoringAgentState:
        """è¯„ä¼°è¯æ®èŠ‚ç‚¹ - ç¬¬äºŒè½®LLMè°ƒç”¨"""
        logger.info("ğŸ“ ç¬¬äºŒè½®åˆ†æï¼šè¯æ®è¯„ä¼°...")
        
        if not self.llm:
            return state
        
        # è·å–ä¹‹å‰çš„åˆ†æç»“æœ
        criteria = state.get("criteria_analysis", {})
        applicant_data = state.get("applicant_data", {})
        
        # æœç´¢ç›¸å…³è¯æ®è¦æ±‚
        evidence_rules = self.kb_manager.search_rules(
            keywords=["è¯æ®", "è¯æ˜", "æ–‡ä»¶"]
        )
        
        evidence_text = self._format_rules_for_llm(evidence_rules)
        
        prompt = f"""
åŸºäºä¹‹å‰çš„åˆ†æç»“æœï¼Œç°åœ¨éœ€è¦è¯„ä¼°ç”³è¯·äººçš„è¯æ®å……åˆ†æ€§ã€‚

ã€ä¹‹å‰çš„åˆ†æç»“æœã€‘
{json.dumps(criteria, ensure_ascii=False, indent=2)}

ã€è¯æ®è¦æ±‚è§„åˆ™ã€‘
{evidence_text}

ã€ç”³è¯·äººæäº¤çš„è¯æ®ã€‘
{json.dumps(applicant_data.get('evidence', {}), ensure_ascii=False, indent=2)}

è¯·è¿›è¡Œä»¥ä¸‹è¯„ä¼°ï¼š
1. ç”³è¯·äººæäº¤çš„è¯æ®æ˜¯å¦å……åˆ†ï¼Ÿ
2. ç¼ºå°‘å“ªäº›å…³é”®è¯æ®ï¼Ÿ
3. ç°æœ‰è¯æ®çš„è´¨é‡å¦‚ä½•ï¼Ÿ
4. å»ºè®®å¦‚ä½•è¡¥å……æˆ–æ”¹è¿›è¯æ®ï¼Ÿ

è¯·ç”¨JSONæ ¼å¼è¿”å›ï¼š
{{
  "evidence_completeness": 0.0-1.0,
  "provided_evidence": {{"è¯æ®ç±»å‹": "è´¨é‡è¯„åˆ†(1-5)"}},
  "missing_evidence": ["ç¼ºå¤±1", "ç¼ºå¤±2", ...],
  "quality_assessment": {{"è¯æ®": "è¯„ä»·"}},
  "improvement_suggestions": ["å»ºè®®1", "å»ºè®®2", ...]
}}
"""
        
        try:
            response = self.llm.invoke(prompt)
            evidence_assessment = json.loads(response.content)
            state["evidence_assessment"] = evidence_assessment
            
            # è®°å½•LLMè°ƒç”¨
            state["llm_calls"].append({
                "stage": "assess_evidence",
                "timestamp": datetime.now().isoformat(),
                "prompt_length": len(prompt),
                "response_length": len(response.content)
            })
            
            state["conversation_history"].append({
                "role": "assistant",
                "content": f"è¯æ®è¯„ä¼°å®Œæˆ: å®Œæ•´æ€§ {evidence_assessment.get('evidence_completeness', 0):.1%}"
            })
            
            logger.info(f"âœ… è¯æ®è¯„ä¼°å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ è¯æ®è¯„ä¼°å¤±è´¥: {e}")
            state["evidence_assessment"] = {"error": str(e)}
        
        state["evaluation_stage"] = "evidence_assessment"
        return state
    
    def _refine_analysis_node(self, state: ScoringAgentState) -> ScoringAgentState:
        """ç²¾ç»†åŒ–åˆ†æèŠ‚ç‚¹ - ç¬¬ä¸‰è½®LLMè°ƒç”¨"""
        logger.info("ğŸ”¬ ç¬¬ä¸‰è½®åˆ†æï¼šç²¾ç»†åŒ–è¯„ä¼°...")
        
        if not self.llm or not state.get("criteria_analysis") or not state.get("evidence_assessment"):
            return state
        
        # ç»¼åˆå‰ä¸¤è½®çš„åˆ†æ
        combined_analysis = {
            "criteria": state["criteria_analysis"],
            "evidence": state["evidence_assessment"]
        }
        
        relevant_rules_text = self._format_rules_for_llm(state["relevant_rules"])
        
        prompt = f"""
åŸºäºå‰ä¸¤è½®çš„åˆ†æï¼Œç°åœ¨éœ€è¦è¿›è¡Œç²¾ç»†åŒ–è¯„ä¼°ã€‚

ã€ç›¸å…³è§„åˆ™ã€‘
{relevant_rules_text}

ã€ä¹‹å‰çš„åˆ†æç»“æœã€‘
{json.dumps(combined_analysis, ensure_ascii=False, indent=2)}

è¯·è¿›è¡Œç²¾ç»†åŒ–åˆ†æï¼š
1. ç»¼åˆæ ‡å‡†ç¬¦åˆæ€§å’Œè¯æ®å……åˆ†æ€§ï¼Œç”³è¯·äººçš„æ•´ä½“èµ„æ ¼å¦‚ä½•ï¼Ÿ
2. åœ¨è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç°äº†å“ªäº›å…³é”®é—®é¢˜æˆ–æ½œåœ¨é£é™©ï¼Ÿ
3. ç”³è¯·äººä¸GTVæ ‡å‡†çš„å¥‘åˆåº¦æ˜¯å¤šå°‘ï¼Ÿ
4. æ˜¯å¦æœ‰éœ€è¦è¿›ä¸€æ­¥æ¾„æ¸…çš„æ–¹é¢ï¼Ÿ

è¯·ç”¨JSONæ ¼å¼è¿”å›ï¼š
{{
  "overall_assessment": "å¼º/ä¸­ç­‰/å¼±",
  "key_issues": ["é—®é¢˜1", "é—®é¢˜2", ...],
  "alignment_score": 0.0-1.0,
  "risk_factors": ["é£é™©1", "é£é™©2", ...],
  "clarification_needed": ["éœ€æ¾„æ¸…1", "éœ€æ¾„æ¸…2", ...],
  "analysis_confidence": 0.0-1.0
}}
"""
        
        try:
            response = self.llm.invoke(prompt)
            refined_analysis = json.loads(response.content)
            
            # åˆå¹¶åˆ°evidence_assessmentä¸­
            state["evidence_assessment"]["refined_analysis"] = refined_analysis
            
            # è®°å½•LLMè°ƒç”¨
            state["llm_calls"].append({
                "stage": "refine_analysis",
                "timestamp": datetime.now().isoformat(),
                "prompt_length": len(prompt),
                "response_length": len(response.content)
            })
            
            state["conversation_history"].append({
                "role": "assistant",
                "content": f"ç²¾ç»†åŒ–åˆ†æå®Œæˆ: æ•´ä½“è¯„ä¼° {refined_analysis.get('overall_assessment', 'N/A')}"
            })
            
            logger.info(f"âœ… ç²¾ç»†åŒ–åˆ†æå®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ ç²¾ç»†åŒ–åˆ†æå¤±è´¥: {e}")
        
        state["evaluation_stage"] = "refine_analysis"
        return state
    
    def _generate_recommendations_node(self, state: ScoringAgentState) -> ScoringAgentState:
        """ç”Ÿæˆå»ºè®®èŠ‚ç‚¹ - ç¬¬å››è½®LLMè°ƒç”¨"""
        logger.info("ğŸ’¡ ç¬¬å››è½®åˆ†æï¼šç”Ÿæˆæ”¹è¿›å»ºè®®...")
        
        if not self.llm:
            return state
        
        # åŸºäºæ‰€æœ‰ä¹‹å‰çš„åˆ†æç”Ÿæˆå»ºè®®
        all_analysis = {
            "criteria": state.get("criteria_analysis", {}),
            "evidence": state.get("evidence_assessment", {})
        }
        
        relevant_rules_text = self._format_rules_for_llm(state["relevant_rules"])
        
        prompt = f"""
åŸºäºå®Œæ•´çš„åˆ†æè¿‡ç¨‹ï¼Œç°åœ¨éœ€è¦ä¸ºç”³è¯·äººç”Ÿæˆå…·ä½“çš„æ”¹è¿›å»ºè®®ã€‚

ã€ç›¸å…³è§„åˆ™ã€‘
{relevant_rules_text}

ã€å®Œæ•´åˆ†æç»“æœã€‘
{json.dumps(all_analysis, ensure_ascii=False, indent=2)}

è¯·ç”Ÿæˆå…·ä½“çš„ã€å¯æ‰§è¡Œçš„æ”¹è¿›å»ºè®®ï¼š
1. ç«‹å³å¯é‡‡å–çš„è¡ŒåŠ¨ï¼ˆçŸ­æœŸï¼‰
2. éœ€è¦é•¿æœŸæŠ•å…¥çš„æ”¹è¿›æ–¹å‘ï¼ˆé•¿æœŸï¼‰
3. å¦‚ä½•æ›´å¥½åœ°å±•ç¤ºç°æœ‰ä¼˜åŠ¿
4. å“ªäº›é¢†åŸŸçš„æ”¹è¿›ä¼šæœ€å¤§åŒ–æˆåŠŸæ¦‚ç‡

è¯·ç”¨JSONæ ¼å¼è¿”å›ï¼š
{{
  "immediate_actions": [
    {{
      "action": "å…·ä½“è¡ŒåŠ¨",
      "impact": "é¢„æœŸå½±å“",
      "priority": "é«˜/ä¸­/ä½",
      "timeline": "å®æ–½æ—¶é—´"
    }},
    ...
  ],
  "long_term_improvements": [
    {{
      "area": "æ”¹è¿›é¢†åŸŸ",
      "goal": "ç›®æ ‡",
      "estimated_timeline": "é¢„è®¡å‘¨æœŸ"
    }},
    ...
  ],
  "leverage_existing_strengths": ["å¦‚ä½•å±•ç¤ºä¼˜åŠ¿1", ...],
  "success_probability_impact": {{
    "improvement": "æ”¹è¿›",
    "estimated_probability_increase": "0.0-1.0"
  }}
}}
"""
        
        try:
            response = self.llm.invoke(prompt)
            recommendations_data = json.loads(response.content)
            
            # æå–å»ºè®®åˆ—è¡¨
            state["recommendations"] = [
                r["action"] for r in recommendations_data.get("immediate_actions", [])
            ]
            state["recommendations"].extend([
                r["area"] for r in recommendations_data.get("long_term_improvements", [])
            ])
            
            # è®°å½•LLMè°ƒç”¨
            state["llm_calls"].append({
                "stage": "generate_recommendations",
                "timestamp": datetime.now().isoformat(),
                "prompt_length": len(prompt),
                "response_length": len(response.content)
            })
            
            state["conversation_history"].append({
                "role": "assistant",
                "content": f"ç”Ÿæˆäº† {len(state['recommendations'])} æ¡å»ºè®®"
            })
            
            logger.info(f"âœ… æ”¹è¿›å»ºè®®ç”Ÿæˆå®Œæˆ: {len(state['recommendations'])} æ¡å»ºè®®")
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆå»ºè®®å¤±è´¥: {e}")
            state["recommendations"] = []
        
        state["evaluation_stage"] = "recommendations"
        return state
    
    def _calculate_score_node(self, state: ScoringAgentState) -> ScoringAgentState:
        """è®¡ç®—æœ€ç»ˆåˆ†æ•°èŠ‚ç‚¹"""
        logger.info("ğŸ¯ è®¡ç®—æœ€ç»ˆåˆ†æ•°...")
        
        # åŸºäºå¤šè½®åˆ†æè®¡ç®—æœ€ç»ˆåˆ†æ•°
        criteria = state.get("criteria_analysis", {})
        evidence = state.get("evidence_assessment", {})
        
        # åˆ†æ•°è®¡ç®—é€»è¾‘
        mc_score = 50 if criteria.get("mc_status") == "æ»¡è¶³" else 0
        oc_count = len(criteria.get("potential_oc", []))
        oc_score = min(50, oc_count * 10)
        evidence_score = evidence.get("evidence_completeness", 0) * 50 if evidence else 0
        
        final_score = (mc_score + oc_score + evidence_score) / 150 * 100
        
        state["final_score"] = final_score
        state["final_reasoning"] = f"""
æœ€ç»ˆè¯„åˆ†: {final_score:.1f}/100

è¯„åˆ†æ„æˆ:
- å¼ºåˆ¶è¦æ±‚ç¬¦åˆæ€§: {mc_score}/50 ({criteria.get('mc_status', 'N/A')})
- å¯é€‰è¦æ±‚è¦†ç›–: {oc_score}/50 (æ»¡è¶³{oc_count}ä¸ª)
- è¯æ®å……åˆ†æ€§: {evidence_score:.0f}/50 ({evidence.get('evidence_completeness', 0):.1%})

å…³é”®å‘ç°:
- ä¼˜åŠ¿: {', '.join(criteria.get('strengths', []))}
- ç¼ºé™·: {', '.join(evidence.get('missing_evidence', []))}
- å»ºè®®: {len(state.get('recommendations', []))}æ¡æ”¹è¿›å»ºè®®

ä¸‹ä¸€æ­¥: {state.get('recommendations', ['æ— '])[0] if state.get('recommendations') else 'æ— '}
"""
        
        state["evaluation_stage"] = "completed"
        
        logger.info(f"âœ… æœ€ç»ˆè¯„åˆ†: {final_score:.1f}/100")
        
        return state
    
    def _format_rules_for_llm(self, rules: List[Dict[str, Any]]) -> str:
        """æ ¼å¼åŒ–è§„åˆ™ä¾›LLMä½¿ç”¨"""
        if not rules:
            return "ï¼ˆæ— ç›¸å…³è§„åˆ™ï¼‰"
        
        formatted = []
        for i, rule in enumerate(rules[:5], 1):  # é™åˆ¶ä¸º5æ¡è§„åˆ™ï¼Œé¿å…è¶…é•¿prompt
            formatted.append(f"""
è§„åˆ™{i}: {rule.get('title', 'N/A')}
åˆ†ç±»: {rule.get('category', 'N/A')} | ç»´åº¦: {rule.get('dimension', 'N/A')}
å†…å®¹æ‘˜è¦: {rule.get('content', '')[:300]}...
""")
        
        return "\n".join(formatted)
    
    def analyze(self, applicant_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´çš„å¤šè½®åˆ†æ"""
        logger.info("=" * 80)
        logger.info("ğŸš€ å¼€å§‹GTVè¯„åˆ†åˆ†ææµç¨‹ï¼ˆLangGraphï¼‰")
        logger.info("=" * 80)
        
        start_time = datetime.now()
        
        # åˆå§‹åŒ–çŠ¶æ€
        initial_state: ScoringAgentState = {
            "applicant_data": applicant_data,
            "current_score": 0.0,
            "evaluation_stage": "initialized",
            "knowledge_base": {"rules": {}, "rule_index": {}},
            "relevant_rules": [],
            "conversation_history": [],
            "llm_calls": [],
            "criteria_analysis": {},
            "evidence_assessment": {},
            "recommendations": [],
            "final_score": 0.0,
            "final_reasoning": "",
            "execution_time": 0.0
        }
        
        try:
            if HAS_LANGGRAPH and self.graph:
                # ä½¿ç”¨LangGraphæ‰§è¡Œ
                logger.info("ä½¿ç”¨LangGraphæ‰§è¡Œæµç¨‹...")
                final_state = self.graph.invoke(initial_state)
            else:
                # å›é€€ï¼šæŒ‰é¡ºåºæ‰§è¡ŒèŠ‚ç‚¹
                logger.info("ä½¿ç”¨é¡ºåºæ‰§è¡Œæ¨¡å¼...")
                final_state = initial_state
                final_state = self._load_kb_node(final_state)
                final_state = self._search_rules_node(final_state)
                final_state = self._analyze_criteria_node(final_state)
                final_state = self._assess_evidence_node(final_state)
                final_state = self._refine_analysis_node(final_state)
                final_state = self._generate_recommendations_node(final_state)
                final_state = self._calculate_score_node(final_state)
        except Exception as e:
            logger.error(f"âŒ åˆ†æè¿‡ç¨‹å‡ºé”™: {e}")
            final_state = initial_state
            final_state["final_reasoning"] = f"åˆ†æå¤±è´¥: {str(e)}"
        
        # è®¡ç®—æ‰§è¡Œæ—¶é—´
        execution_time = (datetime.now() - start_time).total_seconds()
        final_state["execution_time"] = execution_time
        
        logger.info("=" * 80)
        logger.info(f"âœ… åˆ†æå®Œæˆï¼Œæ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’")
        logger.info(f"æœ€ç»ˆè¯„åˆ†: {final_state['final_score']:.1f}/100")
        logger.info(f"LLMè°ƒç”¨æ¬¡æ•°: {len(final_state['llm_calls'])}")
        logger.info("=" * 80)
        
        return {
            "score": final_state["final_score"],
            "reasoning": final_state["final_reasoning"],
            "criteria_analysis": final_state.get("criteria_analysis", {}),
            "evidence_assessment": final_state.get("evidence_assessment", {}),
            "recommendations": final_state.get("recommendations", []),
            "llm_interactions": len(final_state["llm_calls"]),
            "execution_time": execution_time,
            "conversation_history": final_state["conversation_history"]
        }

# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

if __name__ == "__main__":
    # ç¤ºä¾‹ç”³è¯·äººæ•°æ®
    applicant = {
        "name": "å¼ ä¸‰",
        "field": "æ•°å­—æŠ€æœ¯",
        "position": "é¦–å¸­æŠ€æœ¯å®˜",
        "experience_years": 10,
        "evidence": {
            "æ¨èä¿¡": 3,
            "åª’ä½“æŠ¥é“": 5,
            "ä¸“åˆ©": 2,
            "å­¦æœ¯è®ºæ–‡": 4
        }
    }
    
    # åˆ›å»ºAgentå¹¶æ‰§è¡Œåˆ†æ
    agent = LangGraphScoringAgent()
    result = agent.analyze(applicant)
    
    # è¾“å‡ºç»“æœ
    print("\n" + "=" * 80)
    print("åˆ†æç»“æœ")
    print("=" * 80)
    print(json.dumps(result, ensure_ascii=False, indent=2))

