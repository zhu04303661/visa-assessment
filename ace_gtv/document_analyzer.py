#!/usr/bin/env python3
"""
æ–‡æ¡£åˆ†ææ¨¡å— - æå–Excel/Wordå†…å®¹å¹¶ç”¨LLMåˆ†ææç‚¼çŸ¥è¯†è§„åˆ™
"""

import json
import logging
import os
from typing import Any, Dict, List, Optional
from pathlib import Path
from datetime import datetime

try:
    from openpyxl import load_workbook
    HAS_OPENPYXL = True
except ImportError:
    HAS_OPENPYXL = False

try:
    from docx import Document as DocxDocument
    HAS_PYTHON_DOCX = True
except ImportError:
    HAS_PYTHON_DOCX = False

try:
    from langchain_openai import ChatOpenAI
    HAS_LANGCHAIN = True
except ImportError:
    HAS_LANGCHAIN = False

logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO)

# LLM æç¤ºè¯
KNOWLEDGE_EXTRACTION_PROMPT = """
ä½ æ˜¯ä¸€ä½GTVè¯„ä¼°å’ŒçŸ¥è¯†ç®¡ç†ä¸“å®¶ã€‚æˆ‘æä¾›äº†ä¸€ä»½æ–‡æ¡£å†…å®¹ï¼Œå…¶ä¸­åŒ…å«å…³äºç­¾è¯è¯„ä¼°ã€è¯„åˆ†è§„åˆ™æˆ–ä¸“ä¸šæ ‡å‡†çš„ä¿¡æ¯ã€‚

è¯·æ ¹æ®è¿™ä»½å†…å®¹ï¼Œæç‚¼å‡ºå¯ä»¥ç”¨äºGTVè¯„ä¼°ç³»ç»Ÿçš„çŸ¥è¯†è§„åˆ™ã€‚

å¯¹äºæ¯ä¸ªçŸ¥è¯†æ¡ç›®ï¼Œæä¾›ä»¥ä¸‹ç»“æ„åŒ–ä¿¡æ¯ï¼š
{
  "title": "çŸ¥è¯†æ¡ç›®æ ‡é¢˜ï¼ˆç®€æ´ï¼‰",
  "category": "åˆ†ç±»ï¼ˆå¦‚ï¼šè¯„ä¼°æ ‡å‡†ã€è¯„åˆ†è§„åˆ™ã€æ•™è‚²èƒŒæ™¯ã€å·¥ä½œç»éªŒç­‰ï¼‰",
  "dimension": "ç»´åº¦ï¼ˆå¦‚ï¼šeducationã€experienceã€technicalã€leadershipã€impactï¼‰",
  "content": "è¯¦ç»†å†…å®¹æè¿°",
  "scoringRules": ["è§„åˆ™1", "è§„åˆ™2", "..."]
}

æ–‡æ¡£å†…å®¹ï¼š
{document_content}

è¯·è¿”å›ä¸€ä¸ªJSONæ•°ç»„ï¼ŒåŒ…å«ä»æ–‡æ¡£ä¸­æç‚¼å‡ºçš„æ‰€æœ‰çŸ¥è¯†æ¡ç›®ã€‚
ç¡®ä¿æ¯ä¸ªæ¡ç›®éƒ½æ˜¯æœ‰æ•ˆçš„ã€ç›¸å…³çš„ã€å¯ç”¨äºGTVè¯„ä¼°çš„ã€‚
"""

class DocumentExtractor:
    """æ–‡æ¡£å†…å®¹æå–å™¨"""
    
    @staticmethod
    def extract_from_excel(file_path: str) -> str:
        """ä»Excelæ–‡ä»¶æå–æ–‡æœ¬"""
        if not HAS_OPENPYXL:
            raise ImportError("openpyxl æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install openpyxl")
        
        logger.info(f"ğŸ“Š ä»Excelæ–‡ä»¶æå–å†…å®¹: {file_path}")
        
        try:
            wb = load_workbook(file_path)
            content = []
            
            for sheet_name in wb.sheetnames:
                ws = wb[sheet_name]
                content.append(f"\n[å·¥ä½œè¡¨: {sheet_name}]\n")
                
                for row in ws.iter_rows(values_only=True):
                    # è¿‡æ»¤ç©ºè¡Œ
                    values = [str(v) for v in row if v is not None]
                    if values:
                        content.append(" | ".join(values))
            
            text = "\n".join(content)
            logger.info(f"âœ… æˆåŠŸæå–Excelå†…å®¹ï¼Œå…± {len(text)} ä¸ªå­—ç¬¦")
            return text
            
        except Exception as e:
            logger.error(f"âŒ Excelæå–å¤±è´¥: {e}")
            raise
    
    @staticmethod
    def extract_from_word(file_path: str) -> str:
        """ä»Wordæ–‡ä»¶æå–æ–‡æœ¬"""
        if not HAS_PYTHON_DOCX:
            raise ImportError("python-docx æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install python-docx")
        
        logger.info(f"ğŸ“„ ä»Wordæ–‡ä»¶æå–å†…å®¹: {file_path}")
        
        try:
            doc = DocxDocument(file_path)
            content = []
            
            # æå–æ®µè½
            for para in doc.paragraphs:
                if para.text.strip():
                    content.append(para.text)
            
            # æå–è¡¨æ ¼
            for table in doc.tables:
                content.append("\n[è¡¨æ ¼]\n")
                for row in table.rows:
                    cells = [cell.text for cell in row.cells]
                    content.append(" | ".join(cells))
            
            text = "\n".join(content)
            logger.info(f"âœ… æˆåŠŸæå–Wordå†…å®¹ï¼Œå…± {len(text)} ä¸ªå­—ç¬¦")
            return text
            
        except Exception as e:
            logger.error(f"âŒ Wordæå–å¤±è´¥: {e}")
            raise
    
    @staticmethod
    def extract_from_file(file_path: str) -> str:
        """æ ¹æ®æ–‡ä»¶ç±»å‹è‡ªåŠ¨é€‰æ‹©æå–æ–¹æ³•"""
        file_path_obj = Path(file_path)
        suffix = file_path_obj.suffix.lower()
        
        logger.info(f"ğŸ” æ£€æµ‹æ–‡ä»¶ç±»å‹: {suffix}")
        
        if suffix in ['.xlsx', '.xls']:
            return DocumentExtractor.extract_from_excel(file_path)
        elif suffix in ['.docx', '.doc']:
            return DocumentExtractor.extract_from_word(file_path)
        elif suffix == '.txt':
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {suffix}")


class KnowledgeExtractor:
    """çŸ¥è¯†è§„åˆ™æå–å™¨ - ä½¿ç”¨LLMåˆ†ææ–‡æ¡£"""
    
    def __init__(self, api_key: Optional[str] = None):
        """åˆå§‹åŒ–LLM"""
        logger.info("ğŸš€ åˆå§‹åŒ–çŸ¥è¯†æå–å™¨...")
        
        self.api_key = api_key or os.getenv("OPENAI_API_KEY")
        self.llm = None
        
        if HAS_LANGCHAIN and self.api_key:
            try:
                self.llm = ChatOpenAI(
                    api_key=self.api_key,
                    model="gpt-4-turbo-preview",
                    temperature=0.7,
                )
                logger.info("âœ… LLM åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                logger.error(f"âŒ LLMåˆå§‹åŒ–å¤±è´¥: {e}")
                self.llm = None
        else:
            logger.warning("âš ï¸ LLM ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨æœ¬åœ°è§„åˆ™ç”Ÿæˆ")
    
    def extract_knowledge(self, document_content: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨LLMä»æ–‡æ¡£å†…å®¹æå–çŸ¥è¯†è§„åˆ™"""
        logger.info("ğŸ“š å¼€å§‹æå–çŸ¥è¯†è§„åˆ™...")
        
        if not self.llm:
            logger.warning("âš ï¸ LLMä¸å¯ç”¨ï¼Œè¿”å›ç©ºç»“æœ")
            return []
        
        try:
            prompt = KNOWLEDGE_EXTRACTION_PROMPT.format(
                document_content=document_content[:4000]  # é™åˆ¶é•¿åº¦
            )
            
            logger.debug(f"ğŸ“ å‘é€æç¤ºè¯åˆ°LLM...")
            response = self.llm.invoke(prompt)
            
            # è§£æLLMè¿”å›çš„JSON
            response_text = response.content
            logger.debug(f"ğŸ’¬ LLMå“åº”: {response_text[:200]}...")
            
            # å°è¯•æå–JSON
            import re
            json_match = re.search(r'\[.*\]', response_text, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                knowledge_items = json.loads(json_str)
                logger.info(f"âœ… æˆåŠŸæå– {len(knowledge_items)} ä¸ªçŸ¥è¯†æ¡ç›®")
                return knowledge_items
            else:
                logger.warning("âš ï¸ LLMå“åº”ä¸­æœªæ‰¾åˆ°JSONæ ¼å¼æ•°æ®")
                return []
                
        except Exception as e:
            logger.error(f"âŒ çŸ¥è¯†æå–å¤±è´¥: {e}")
            return []
    
    def analyze_and_extract(self, file_path: str) -> Dict[str, Any]:
        """å®Œæ•´æµç¨‹ï¼šæå–æ–‡ä»¶å†…å®¹ â†’ LLMåˆ†æ â†’ æç‚¼çŸ¥è¯†"""
        logger.info(f"\n{'='*80}")
        logger.info(f"ğŸ“– å¼€å§‹åˆ†ææ–‡æ¡£: {file_path}")
        logger.info(f"{'='*80}")
        
        start_time = datetime.now()
        
        try:
            # ç¬¬ä¸€æ­¥ï¼šæå–æ–‡ä»¶å†…å®¹
            logger.info("ç¬¬1æ­¥: æå–æ–‡ä»¶å†…å®¹...")
            content = DocumentExtractor.extract_from_file(file_path)
            
            # ç¬¬äºŒæ­¥ï¼šLLMåˆ†æå’Œæå–çŸ¥è¯†
            logger.info("ç¬¬2æ­¥: LLMåˆ†æå’Œæå–çŸ¥è¯†è§„åˆ™...")
            knowledge_items = self.extract_knowledge(content)
            
            # ç¬¬ä¸‰æ­¥ï¼šéªŒè¯å’Œè¡¥å……ä¿¡æ¯
            logger.info("ç¬¬3æ­¥: éªŒè¯å’Œè¡¥å……çŸ¥è¯†æ¡ç›®...")
            validated_items = self._validate_items(knowledge_items)
            
            elapsed = (datetime.now() - start_time).total_seconds()
            
            result = {
                "success": True,
                "file": Path(file_path).name,
                "file_size": os.path.getsize(file_path),
                "content_length": len(content),
                "items_count": len(validated_items),
                "items": validated_items,
                "analysis_time": f"{elapsed:.2f}s",
                "timestamp": datetime.now().isoformat(),
            }
            
            logger.info(f"âœ… åˆ†æå®Œæˆï¼")
            logger.info(f"   - æ–‡ä»¶: {result['file']}")
            logger.info(f"   - æå–çš„çŸ¥è¯†æ¡ç›®: {result['items_count']}")
            logger.info(f"   - è€—æ—¶: {result['analysis_time']}")
            logger.info(f"{'='*80}\n")
            
            return result
            
        except Exception as e:
            logger.error(f"âŒ åˆ†æå¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e),
                "file": Path(file_path).name if file_path else "unknown",
                "timestamp": datetime.now().isoformat(),
            }
    
    @staticmethod
    def _validate_items(items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """éªŒè¯å’Œè¡¥å……çŸ¥è¯†æ¡ç›®"""
        validated = []
        
        for item in items:
            # æ£€æŸ¥å¿…éœ€å­—æ®µ
            if not item.get("title") or not item.get("content"):
                logger.warning(f"âš ï¸ è·³è¿‡ä¸å®Œæ•´çš„æ¡ç›®: {item.get('title', 'Unknown')}")
                continue
            
            # è¡¥å……ç¼ºå¤±å­—æ®µ
            validated_item = {
                "id": f"kb-{int(datetime.now().timestamp()*1000)}-{len(validated)}",
                "title": item.get("title", "æœªå‘½å"),
                "category": item.get("category", "å…¶ä»–"),
                "dimension": item.get("dimension", ""),
                "content": item.get("content", ""),
                "scoringRules": item.get("scoringRules", []),
                "createdAt": datetime.now().isoformat(),
                "source": "document_analysis",
            }
            
            validated.append(validated_item)
            logger.debug(f"âœ“ éªŒè¯é€šè¿‡: {validated_item['title']}")
        
        return validated


# æµ‹è¯•å‡½æ•°
def test_document_analyzer():
    """æµ‹è¯•æ–‡æ¡£åˆ†æå™¨"""
    logger.info("\n" + "â–ˆ"*80)
    logger.info("â–ˆ  æ–‡æ¡£åˆ†æå™¨ - åŠŸèƒ½æµ‹è¯•")
    logger.info("â–ˆ"*80)
    
    analyzer = KnowledgeExtractor()
    
    # æ¨¡æ‹Ÿæµ‹è¯•ï¼ˆä½¿ç”¨æœ¬åœ°è§„åˆ™ï¼‰
    test_content = """
    GTVè¯„ä¼°æ ‡å‡†ï¼š
    1. æ•™è‚²èƒŒæ™¯ - ç”³è¯·äººéœ€è¦å…·æœ‰ç¡•å£«æˆ–ä»¥ä¸Šå­¦ä½ï¼Œæœ€å¥½æ¥è‡ªé¡¶çº§å¤§å­¦
    2. å·¥ä½œç»éªŒ - éœ€è¦è‡³å°‘5å¹´ç›¸å…³å·¥ä½œç»éªŒï¼Œåœ¨è¡Œä¸šä¸­æœ‰è®¤å¯åº¦
    3. æŠ€æœ¯ä¸“é•¿ - å¯¹äºæŠ€æœ¯ç±»ç”³è¯·ï¼Œéœ€è¦å±•ç¤ºæ·±åº¦çš„æŠ€æœ¯èƒ½åŠ›å’Œåˆ›æ–°
    
    è¯„åˆ†è§„åˆ™ï¼š
    - å­¦ä½ç­‰çº§ï¼šåšå£«100åˆ†ï¼Œç¡•å£«80åˆ†ï¼Œå­¦å£«50åˆ†
    - å·¥ä½œå¹´é™ï¼š15+å¹´100åˆ†ï¼Œ10-15å¹´90åˆ†ï¼Œ5-10å¹´70åˆ†
    - è¡Œä¸šå½±å“åŠ›ï¼šå›½é™…çŸ¥å100åˆ†ï¼Œè¡Œä¸šé¢†è¢–90åˆ†ï¼ŒåŒºåŸŸå½±å“70åˆ†
    """
    
    logger.info("\næµ‹è¯•LLMåˆ†æåŠŸèƒ½...")
    # æ³¨æ„ï¼šå®é™…ä½¿ç”¨éœ€è¦APIå¯†é’¥
    logger.info("âš ï¸ æµ‹è¯•æ¨¡å¼ - éœ€è¦è®¾ç½® OPENAI_API_KEY æ‰èƒ½å®Œæ•´è¿è¡Œ")
    
    logger.info("\n" + "â–ˆ"*80)
    logger.info("â–ˆ  æµ‹è¯•å®Œæˆ")
    logger.info("â–ˆ"*80)


if __name__ == "__main__":
    test_document_analyzer()
